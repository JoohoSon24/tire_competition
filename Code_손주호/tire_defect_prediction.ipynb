{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tire Defect Prediction - Hybrid Model (Boosting + Deep Learning)\n",
    "## Expert Manufacturing AI Solution for Smart Factory\n",
    "\n",
    "**Objective:** Binary Classification of Tire Defects (NG vs Good)\n",
    "\n",
    "**Approach:** Hybrid Stacking/Ensemble Model combining:\n",
    "- Branch 1: XGBoost/CatBoost for Design/Process Features\n",
    "- Branch 2: Deep Neural Network for Simulation Features\n",
    "- Fusion: MLP Head for Final Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Boosting Libraries\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"[INFO] Loading training data...\")\n",
    "df_train = pd.read_csv('train.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df_train.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df_train.head())\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(df_train['Label Class'].value_counts())\n",
    "print(f\"\\nClass Balance:\")\n",
    "print(df_train['Label Class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering (Modular Structure)\n",
    "### 3.1 Define Feature Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "DESIGN_FEATURES = ['Mass_Pilot', 'Width', 'Aspect', 'Inch', 'Plant'] + \\\n",
    "                  [f'Proc_Param{i}' for i in range(1, 12)]\n",
    "\n",
    "SIMULATION_FEATURES = [f'x{i}' for i in range(1, 6)] + \\\n",
    "                      [f'y{i}' for i in range(1, 6)] + \\\n",
    "                      [f'x{i}' for i in range(0, 256)] + \\\n",
    "                      [f'y{i}' for i in range(0, 256)] + \\\n",
    "                      [f'p{i}' for i in range(0, 256)] + \\\n",
    "                      [f'G{i}' for i in range(1, 5)]\n",
    "\n",
    "# Remove duplicates (x0-x5, y0-y5 might overlap)\n",
    "SIMULATION_FEATURES = list(set(SIMULATION_FEATURES))\n",
    "SIMULATION_FEATURES.sort()\n",
    "\n",
    "TARGET = 'Label Class'\n",
    "\n",
    "print(f\"Design Features: {len(DESIGN_FEATURES)}\")\n",
    "print(f\"Simulation Features: {len(SIMULATION_FEATURES)}\")\n",
    "print(f\"Total Features: {len(DESIGN_FEATURES) + len(SIMULATION_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modular Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePreprocessor:\n",
    "    \"\"\"\n",
    "    Modular preprocessing pipeline for tire defect prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.label_encoders = {}\n",
    "        self.scalers = {}\n",
    "        self.design_features = None\n",
    "        self.simulation_features = None\n",
    "        \n",
    "    def _print(self, message):\n",
    "        \"\"\"Print if verbose mode is enabled.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(message)\n",
    "    \n",
    "    def handle_missing_values(self, df, strategy='median'):\n",
    "        \"\"\"\n",
    "        Step 1: Handle missing values\n",
    "        \"\"\"\n",
    "        self._print(\"\\n[1] Currently processing: Missing Value Handling...\")\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Check missing values\n",
    "        missing_counts = df_processed.isnull().sum()\n",
    "        missing_features = missing_counts[missing_counts > 0]\n",
    "        \n",
    "        if len(missing_features) > 0:\n",
    "            self._print(f\"   - Found {len(missing_features)} features with missing values\")\n",
    "            for feature in missing_features.index:\n",
    "                if df_processed[feature].dtype in ['float64', 'int64']:\n",
    "                    if strategy == 'median':\n",
    "                        fill_value = df_processed[feature].median()\n",
    "                    elif strategy == 'mean':\n",
    "                        fill_value = df_processed[feature].mean()\n",
    "                    else:\n",
    "                        fill_value = 0\n",
    "                    df_processed[feature].fillna(fill_value, inplace=True)\n",
    "                else:\n",
    "                    # Categorical: fill with mode or 'Unknown'\n",
    "                    df_processed[feature].fillna('Unknown', inplace=True)\n",
    "            self._print(f\"   - Missing values handled using {strategy} strategy\")\n",
    "        else:\n",
    "            self._print(\"   - No missing values found\")\n",
    "        \n",
    "        self._print(\"   ✓ Missing value handling complete\")\n",
    "        return df_processed\n",
    "    \n",
    "    def encode_categorical(self, df, categorical_features, fit=True):\n",
    "        \"\"\"\n",
    "        Step 2: Encode categorical features\n",
    "        \"\"\"\n",
    "        self._print(\"\\n[2] Currently processing: Categorical Encoding...\")\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        for feature in categorical_features:\n",
    "            if feature not in df_processed.columns:\n",
    "                continue\n",
    "                \n",
    "            if fit:\n",
    "                le = LabelEncoder()\n",
    "                df_processed[feature] = le.fit_transform(df_processed[feature].astype(str))\n",
    "                self.label_encoders[feature] = le\n",
    "                self._print(f\"   - Encoded {feature}: {len(le.classes_)} unique values\")\n",
    "            else:\n",
    "                le = self.label_encoders[feature]\n",
    "                # Handle unseen categories\n",
    "                df_processed[feature] = df_processed[feature].astype(str).map(\n",
    "                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                )\n",
    "        \n",
    "        self._print(\"   ✓ Categorical encoding complete\")\n",
    "        return df_processed\n",
    "    \n",
    "    def scale_features(self, df, feature_groups, fit=True):\n",
    "        \"\"\"\n",
    "        Step 3: Scale numerical features by group\n",
    "        \"\"\"\n",
    "        self._print(\"\\n[3] Currently processing: Feature Scaling...\")\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        for group_name, features in feature_groups.items():\n",
    "            # Filter features that exist in dataframe\n",
    "            existing_features = [f for f in features if f in df_processed.columns]\n",
    "            \n",
    "            if len(existing_features) == 0:\n",
    "                continue\n",
    "            \n",
    "            if fit:\n",
    "                scaler = StandardScaler()\n",
    "                df_processed[existing_features] = scaler.fit_transform(\n",
    "                    df_processed[existing_features]\n",
    "                )\n",
    "                self.scalers[group_name] = scaler\n",
    "                self._print(f\"   - Scaled {group_name}: {len(existing_features)} features\")\n",
    "            else:\n",
    "                scaler = self.scalers[group_name]\n",
    "                df_processed[existing_features] = scaler.transform(\n",
    "                    df_processed[existing_features]\n",
    "                )\n",
    "        \n",
    "        self._print(\"   ✓ Feature scaling complete\")\n",
    "        return df_processed\n",
    "    \n",
    "    def fit_transform(self, df, design_features, simulation_features, target_col):\n",
    "        \"\"\"\n",
    "        Main preprocessing pipeline (fit and transform)\n",
    "        \"\"\"\n",
    "        self._print(\"\\n\" + \"=\"*60)\n",
    "        self._print(\"STARTING FEATURE ENGINEERING PIPELINE (FIT)\")\n",
    "        self._print(\"=\"*60)\n",
    "        \n",
    "        self.design_features = design_features\n",
    "        self.simulation_features = simulation_features\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Step 1: Handle missing values\n",
    "        df_processed = self.handle_missing_values(df_processed, strategy='median')\n",
    "        \n",
    "        # Step 2: Encode categorical features\n",
    "        categorical_features = ['Mass_Pilot', 'Plant']  # Add others if needed\n",
    "        df_processed = self.encode_categorical(df_processed, categorical_features, fit=True)\n",
    "        \n",
    "        # Step 3: Scale features by group\n",
    "        feature_groups = {\n",
    "            'design': design_features,\n",
    "            'simulation': simulation_features\n",
    "        }\n",
    "        df_processed = self.scale_features(df_processed, feature_groups, fit=True)\n",
    "        \n",
    "        self._print(\"\\n\" + \"=\"*60)\n",
    "        self._print(\"FEATURE ENGINEERING PIPELINE COMPLETE\")\n",
    "        self._print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Apply fitted preprocessing (transform only)\n",
    "        \"\"\"\n",
    "        self._print(\"\\n\" + \"=\"*60)\n",
    "        self._print(\"APPLYING FEATURE ENGINEERING PIPELINE (TRANSFORM)\")\n",
    "        self._print(\"=\"*60)\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Step 1: Handle missing values\n",
    "        df_processed = self.handle_missing_values(df_processed, strategy='median')\n",
    "        \n",
    "        # Step 2: Encode categorical features\n",
    "        categorical_features = list(self.label_encoders.keys())\n",
    "        df_processed = self.encode_categorical(df_processed, categorical_features, fit=False)\n",
    "        \n",
    "        # Step 3: Scale features\n",
    "        feature_groups = {\n",
    "            'design': self.design_features,\n",
    "            'simulation': self.simulation_features\n",
    "        }\n",
    "        df_processed = self.scale_features(df_processed, feature_groups, fit=False)\n",
    "        \n",
    "        self._print(\"\\n\" + \"=\"*60)\n",
    "        self._print(\"FEATURE ENGINEERING PIPELINE COMPLETE\")\n",
    "        self._print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target to binary (NG=1, Good=0)\n",
    "df_train['Label Class'] = (df_train['Label Class'] == 'NG').astype(int)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = FeaturePreprocessor(verbose=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_processed = preprocessor.fit_transform(\n",
    "    df_train,\n",
    "    design_features=DESIGN_FEATURES,\n",
    "    simulation_features=SIMULATION_FEATURES,\n",
    "    target_col=TARGET\n",
    ")\n",
    "\n",
    "# Split features and target\n",
    "X = df_processed.drop(columns=[TARGET])\n",
    "y = df_processed[TARGET]\n",
    "\n",
    "print(f\"\\nFinal processed dataset shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection Using Boosting\n",
    "### 4.1 Train Temporary Boosting Model for Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SELECTION USING BOOSTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data for feature selection\n",
    "X_train_fs, X_val_fs, y_train_fs, y_val_fs = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n[INFO] Training temporary XGBoost model for feature importance...\")\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_fs = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED,\n",
    "    tree_method='hist',\n",
    "    enable_categorical=False\n",
    ")\n",
    "\n",
    "xgb_fs.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_fs.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\n[INFO] Top 20 Most Important Features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feature_importance.head(30)['feature'][::-1], \n",
    "         feature_importance.head(30)['importance'][::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 30 Feature Importance (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Select Top N Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top N features or features above threshold\n",
    "TOP_N_FEATURES = 100  # Adjust based on performance\n",
    "IMPORTANCE_THRESHOLD = 0.001  # Alternative: use threshold\n",
    "\n",
    "# Method 1: Top N features\n",
    "selected_features = feature_importance.head(TOP_N_FEATURES)['feature'].tolist()\n",
    "\n",
    "# Method 2: Features above threshold (uncomment to use)\n",
    "# selected_features = feature_importance[\n",
    "#     feature_importance['importance'] > IMPORTANCE_THRESHOLD\n",
    "# ]['feature'].tolist()\n",
    "\n",
    "print(f\"\\n[INFO] Selected {len(selected_features)} features for final model\")\n",
    "\n",
    "# Separate selected features into design and simulation groups\n",
    "selected_design_features = [f for f in selected_features if f in DESIGN_FEATURES]\n",
    "selected_simulation_features = [f for f in selected_features if f in SIMULATION_FEATURES]\n",
    "\n",
    "print(f\"   - Design features selected: {len(selected_design_features)}\")\n",
    "print(f\"   - Simulation features selected: {len(selected_simulation_features)}\")\n",
    "\n",
    "# Create filtered datasets\n",
    "X_selected = X[selected_features]\n",
    "X_design = X[selected_design_features]\n",
    "X_simulation = X[selected_simulation_features]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SELECTION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Model Architecture\n",
    "### 5.1 Define Deep Neural Network Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationDNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Neural Network for Simulation Features\n",
    "    Returns both latent representations and predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim=64, dropout_rate=0.3):\n",
    "        super(SimulationDNN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)  # h2\n",
    "        prediction = self.predictor(latent)  # p2 (logits)\n",
    "        return latent, prediction\n",
    "\n",
    "\n",
    "class HybridFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid Model combining Boosting and DNN outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, boosting_pred_dim, boosting_latent_dim, \n",
    "                 dnn_latent_dim, dropout_rate=0.3):\n",
    "        super(HybridFusionModel, self).__init__()\n",
    "        \n",
    "        # Total input dimension: p1 + h1 + h2 + p2\n",
    "        total_dim = boosting_pred_dim + boosting_latent_dim + dnn_latent_dim + 1\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(total_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, boosting_pred, boosting_latent, dnn_latent, dnn_pred):\n",
    "        # Concatenate all inputs\n",
    "        fused = torch.cat([boosting_pred, boosting_latent, dnn_latent, dnn_pred], dim=1)\n",
    "        output = self.fusion(fused)\n",
    "        return output\n",
    "\n",
    "\n",
    "print(\"[INFO] Neural network architectures defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Data for Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "(\n",
    "    X_design_train, X_design_val,\n",
    "    X_sim_train, X_sim_val,\n",
    "    y_train, y_val\n",
    ") = train_test_split(\n",
    "    X_design, X_simulation, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(y_train)}\")\n",
    "print(f\"Validation set size: {len(y_val)}\")\n",
    "print(f\"\\nDesign features shape: {X_design_train.shape}\")\n",
    "print(f\"Simulation features shape: {X_sim_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train Branch 1: Boosting Model (Design Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING BRANCH 1: XGBoost (Design Features)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train XGBoost model on design features\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=SEED,\n",
    "    tree_method='hist',\n",
    "    enable_categorical=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "print(\"[INFO] Training XGBoost model...\")\n",
    "xgb_model.fit(\n",
    "    X_design_train, y_train,\n",
    "    eval_set=[(X_design_val, y_val)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Get predictions and leaf indices (as latent representation)\n",
    "print(\"\\n[INFO] Extracting XGBoost outputs...\")\n",
    "xgb_pred_train = xgb_model.predict_proba(X_design_train)[:, 1].reshape(-1, 1)\n",
    "xgb_pred_val = xgb_model.predict_proba(X_design_val)[:, 1].reshape(-1, 1)\n",
    "\n",
    "# Get leaf indices as latent features (h1)\n",
    "xgb_latent_train = xgb_model.apply(X_design_train)  # Shape: (n_samples, n_estimators)\n",
    "xgb_latent_val = xgb_model.apply(X_design_val)\n",
    "\n",
    "print(f\"   - XGBoost predictions shape: {xgb_pred_train.shape}\")\n",
    "print(f\"   - XGBoost latent features shape: {xgb_latent_train.shape}\")\n",
    "\n",
    "# Evaluate XGBoost performance\n",
    "xgb_val_pred_binary = (xgb_pred_val > 0.5).astype(int)\n",
    "print(f\"\\n[RESULTS] XGBoost Branch Performance:\")\n",
    "print(f\"   - Validation Accuracy: {accuracy_score(y_val, xgb_val_pred_binary):.4f}\")\n",
    "print(f\"   - Validation ROC-AUC: {roc_auc_score(y_val, xgb_pred_val):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BRANCH 1 TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Train Branch 2: DNN Model (Simulation Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING BRANCH 2: DNN (Simulation Features)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_sim_train_tensor = torch.FloatTensor(X_sim_train.values).to(device)\n",
    "X_sim_val_tensor = torch.FloatTensor(X_sim_val.values).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1).to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_sim_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_sim_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize DNN model\n",
    "dnn_model = SimulationDNN(\n",
    "    input_dim=X_sim_train.shape[1],\n",
    "    latent_dim=64,\n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(dnn_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n[INFO] Training DNN model...\")\n",
    "n_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    dnn_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        latent, pred = dnn_model(batch_X)\n",
    "        loss = criterion(pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    dnn_model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            latent, pred = dnn_model(batch_X)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(dnn_model.state_dict(), 'best_dnn_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n[INFO] Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "dnn_model.load_state_dict(torch.load('best_dnn_model.pth'))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DNN Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Extract DNN outputs\n",
    "print(\"\\n[INFO] Extracting DNN outputs...\")\n",
    "dnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    dnn_latent_train, dnn_pred_train = dnn_model(X_sim_train_tensor)\n",
    "    dnn_latent_val, dnn_pred_val = dnn_model(X_sim_val_tensor)\n",
    "\n",
    "print(f\"   - DNN latent features shape: {dnn_latent_train.shape}\")\n",
    "print(f\"   - DNN predictions shape: {dnn_pred_train.shape}\")\n",
    "\n",
    "# Evaluate DNN performance\n",
    "dnn_val_pred_prob = torch.sigmoid(dnn_pred_val).cpu().numpy()\n",
    "dnn_val_pred_binary = (dnn_val_pred_prob > 0.5).astype(int)\n",
    "print(f\"\\n[RESULTS] DNN Branch Performance:\")\n",
    "print(f\"   - Validation Accuracy: {accuracy_score(y_val, dnn_val_pred_binary):.4f}\")\n",
    "print(f\"   - Validation ROC-AUC: {roc_auc_score(y_val, dnn_val_pred_prob):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BRANCH 2 TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING FUSION MODEL (Final Layer)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare fusion inputs\n",
    "# p1: XGBoost predictions\n",
    "xgb_pred_train_tensor = torch.FloatTensor(xgb_pred_train).to(device)\n",
    "xgb_pred_val_tensor = torch.FloatTensor(xgb_pred_val).to(device)\n",
    "\n",
    "# h1: XGBoost latent (leaf indices) - normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_xgb_latent = StandardScaler()\n",
    "xgb_latent_train_scaled = scaler_xgb_latent.fit_transform(xgb_latent_train)\n",
    "xgb_latent_val_scaled = scaler_xgb_latent.transform(xgb_latent_val)\n",
    "\n",
    "xgb_latent_train_tensor = torch.FloatTensor(xgb_latent_train_scaled).to(device)\n",
    "xgb_latent_val_tensor = torch.FloatTensor(xgb_latent_val_scaled).to(device)\n",
    "\n",
    "# h2 and p2 are already tensors from DNN\n",
    "\n",
    "# Create fusion datasets\n",
    "fusion_train_dataset = TensorDataset(\n",
    "    xgb_pred_train_tensor,\n",
    "    xgb_latent_train_tensor,\n",
    "    dnn_latent_train,\n",
    "    dnn_pred_train,\n",
    "    y_train_tensor\n",
    ")\n",
    "\n",
    "fusion_val_dataset = TensorDataset(\n",
    "    xgb_pred_val_tensor,\n",
    "    xgb_latent_val_tensor,\n",
    "    dnn_latent_val,\n",
    "    dnn_pred_val,\n",
    "    y_val_tensor\n",
    ")\n",
    "\n",
    "fusion_train_loader = DataLoader(fusion_train_dataset, batch_size=64, shuffle=True)\n",
    "fusion_val_loader = DataLoader(fusion_val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Initialize fusion model\n",
    "fusion_model = HybridFusionModel(\n",
    "    boosting_pred_dim=1,\n",
    "    boosting_latent_dim=xgb_latent_train.shape[1],\n",
    "    dnn_latent_dim=64,\n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion_fusion = nn.BCEWithLogitsLoss()\n",
    "optimizer_fusion = optim.Adam(fusion_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler_fusion = optim.lr_scheduler.ReduceLROnPlateau(optimizer_fusion, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n[INFO] Training fusion model...\")\n",
    "n_epochs_fusion = 80\n",
    "best_val_loss_fusion = float('inf')\n",
    "patience_fusion = 15\n",
    "patience_counter_fusion = 0\n",
    "\n",
    "fusion_train_losses = []\n",
    "fusion_val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs_fusion):\n",
    "    # Training phase\n",
    "    fusion_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for xgb_p, xgb_h, dnn_h, dnn_p, batch_y in fusion_train_loader:\n",
    "        optimizer_fusion.zero_grad()\n",
    "        output = fusion_model(xgb_p, xgb_h, dnn_h, dnn_p)\n",
    "        loss = criterion_fusion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_fusion.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(fusion_train_loader)\n",
    "    fusion_train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    fusion_model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xgb_p, xgb_h, dnn_h, dnn_p, batch_y in fusion_val_loader:\n",
    "            output = fusion_model(xgb_p, xgb_h, dnn_h, dnn_p)\n",
    "            loss = criterion_fusion(output, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(fusion_val_loader)\n",
    "    fusion_val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler_fusion.step(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs_fusion}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss_fusion:\n",
    "        best_val_loss_fusion = val_loss\n",
    "        patience_counter_fusion = 0\n",
    "        # Save best model\n",
    "        torch.save(fusion_model.state_dict(), 'best_fusion_model.pth')\n",
    "    else:\n",
    "        patience_counter_fusion += 1\n",
    "        if patience_counter_fusion >= patience_fusion:\n",
    "            print(f\"\\n[INFO] Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "fusion_model.load_state_dict(torch.load('best_fusion_model.pth'))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fusion_train_losses, label='Train Loss')\n",
    "plt.plot(fusion_val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Fusion Model Training History')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FUSION MODEL TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "### 6.1 Generate Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get final predictions on validation set\n",
    "fusion_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred_logits = fusion_model(\n",
    "        xgb_pred_val_tensor,\n",
    "        xgb_latent_val_tensor,\n",
    "        dnn_latent_val,\n",
    "        dnn_pred_val\n",
    "    )\n",
    "    final_pred_probs = torch.sigmoid(final_pred_logits).cpu().numpy()\n",
    "\n",
    "final_pred_binary = (final_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_val, final_pred_binary)\n",
    "precision = precision_score(y_val, final_pred_binary)\n",
    "recall = recall_score(y_val, final_pred_binary)\n",
    "f1 = f1_score(y_val, final_pred_binary)\n",
    "roc_auc = roc_auc_score(y_val, final_pred_probs)\n",
    "\n",
    "print(\"\\n[FINAL RESULTS] Hybrid Model Performance:\")\n",
    "print(f\"   - Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"   - Precision: {precision:.4f}\")\n",
    "print(f\"   - Recall:    {recall:.4f}\")\n",
    "print(f\"   - F1-Score:  {f1:.4f}\")\n",
    "print(f\"   - ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_val, final_pred_binary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Hybrid Model')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n[CLASSIFICATION REPORT]\")\n",
    "print(classification_report(y_val, final_pred_binary, target_names=['Good', 'NG']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compare Individual Models vs Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Model': ['XGBoost (Design)', 'DNN (Simulation)', 'Hybrid Fusion'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_val, xgb_val_pred_binary),\n",
    "        accuracy_score(y_val, dnn_val_pred_binary),\n",
    "        accuracy\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_val, xgb_pred_val),\n",
    "        roc_auc_score(y_val, dnn_val_pred_prob),\n",
    "        roc_auc\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_val, xgb_val_pred_binary),\n",
    "        f1_score(y_val, dnn_val_pred_binary),\n",
    "        f1\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n[MODEL COMPARISON]\")\n",
    "print(comparison_results.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "metrics = ['Accuracy', 'ROC-AUC', 'F1-Score']\n",
    "for idx, metric in enumerate(metrics):\n",
    "    axes[idx].bar(comparison_results['Model'], comparison_results[metric])\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].set_title(f'{metric} Comparison')\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].tick_params(axis='x', rotation=15)\n",
    "    for i, v in enumerate(comparison_results[metric]):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Pipeline for New Data\n",
    "### 7.1 Create End-to-End Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tire_defects(df_new, preprocessor, xgb_model, dnn_model, fusion_model, \n",
    "                         selected_design_features, selected_simulation_features,\n",
    "                         scaler_xgb_latent, device):\n",
    "    \"\"\"\n",
    "    End-to-end prediction pipeline for new tire data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_new : pd.DataFrame\n",
    "        New data to predict (without target column)\n",
    "    preprocessor : FeaturePreprocessor\n",
    "        Fitted preprocessor object\n",
    "    xgb_model : XGBClassifier\n",
    "        Trained XGBoost model\n",
    "    dnn_model : SimulationDNN\n",
    "        Trained DNN model\n",
    "    fusion_model : HybridFusionModel\n",
    "        Trained fusion model\n",
    "    selected_design_features : list\n",
    "        List of selected design feature names\n",
    "    selected_simulation_features : list\n",
    "        List of selected simulation feature names\n",
    "    scaler_xgb_latent : StandardScaler\n",
    "        Fitted scaler for XGBoost latent features\n",
    "    device : torch.device\n",
    "        Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : np.ndarray\n",
    "        Predicted probabilities of defect (NG)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"[INFO] Running prediction pipeline...\")\n",
    "    \n",
    "    # Step 1: Preprocess\n",
    "    df_processed = preprocessor.transform(df_new)\n",
    "    \n",
    "    # Step 2: Extract selected features\n",
    "    X_design = df_processed[selected_design_features]\n",
    "    X_simulation = df_processed[selected_simulation_features]\n",
    "    \n",
    "    # Step 3: Get XGBoost outputs\n",
    "    xgb_pred = xgb_model.predict_proba(X_design)[:, 1].reshape(-1, 1)\n",
    "    xgb_latent = xgb_model.apply(X_design)\n",
    "    xgb_latent_scaled = scaler_xgb_latent.transform(xgb_latent)\n",
    "    \n",
    "    # Step 4: Get DNN outputs\n",
    "    X_sim_tensor = torch.FloatTensor(X_simulation.values).to(device)\n",
    "    dnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        dnn_latent, dnn_pred = dnn_model(X_sim_tensor)\n",
    "    \n",
    "    # Step 5: Get fusion predictions\n",
    "    xgb_pred_tensor = torch.FloatTensor(xgb_pred).to(device)\n",
    "    xgb_latent_tensor = torch.FloatTensor(xgb_latent_scaled).to(device)\n",
    "    \n",
    "    fusion_model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_logits = fusion_model(xgb_pred_tensor, xgb_latent_tensor, dnn_latent, dnn_pred)\n",
    "        final_probs = torch.sigmoid(final_logits).cpu().numpy()\n",
    "    \n",
    "    print(\"[INFO] Prediction complete\")\n",
    "    return final_probs\n",
    "\n",
    "\n",
    "# Example usage (uncomment when you have test data)\n",
    "# df_test = pd.read_csv('test.csv')\n",
    "# test_predictions = predict_tire_defects(\n",
    "#     df_test, preprocessor, xgb_model, dnn_model, fusion_model,\n",
    "#     selected_design_features, selected_simulation_features,\n",
    "#     scaler_xgb_latent, device\n",
    "# )\n",
    "# \n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': df_test['id'],  # Adjust column name as needed\n",
    "#     'prediction': test_predictions.flatten()\n",
    "# })\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n[INFO] Prediction pipeline function created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODELS AND ARTIFACTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save preprocessor\n",
    "with open('preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "print(\"[SAVED] preprocessor.pkl\")\n",
    "\n",
    "# Save XGBoost model\n",
    "xgb_model.save_model('xgb_model.json')\n",
    "print(\"[SAVED] xgb_model.json\")\n",
    "\n",
    "# Save XGBoost latent scaler\n",
    "with open('scaler_xgb_latent.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_xgb_latent, f)\n",
    "print(\"[SAVED] scaler_xgb_latent.pkl\")\n",
    "\n",
    "# Save feature lists\n",
    "feature_config = {\n",
    "    'selected_design_features': selected_design_features,\n",
    "    'selected_simulation_features': selected_simulation_features,\n",
    "    'all_design_features': DESIGN_FEATURES,\n",
    "    'all_simulation_features': SIMULATION_FEATURES\n",
    "}\n",
    "\n",
    "with open('feature_config.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_config, f)\n",
    "print(\"[SAVED] feature_config.pkl\")\n",
    "\n",
    "# Neural network models are already saved as:\n",
    "# - best_dnn_model.pth\n",
    "# - best_fusion_model.pth\n",
    "\n",
    "print(\"\\n[INFO] All models and artifacts saved successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROJECT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### What We've Built:\n",
    "1. **Feature Engineering Pipeline**: Modular preprocessing with verbose logging\n",
    "2. **Feature Selection**: Using XGBoost importance to select top features\n",
    "3. **Hybrid Model Architecture**:\n",
    "   - Branch 1: XGBoost for Design/Process parameters\n",
    "   - Branch 2: Deep Neural Network for Simulation features\n",
    "   - Fusion Layer: MLP combining both branches for final prediction\n",
    "\n",
    "### Key Advantages:\n",
    "- **Ensemble Strength**: Combines tree-based and neural network approaches\n",
    "- **Feature-Specific Learning**: Different model types for different feature groups\n",
    "- **Interpretability**: XGBoost provides feature importance, fusion weights show contribution\n",
    "- **Robustness**: Multiple layers of learning reduce overfitting\n",
    "\n",
    "### Next Steps:\n",
    "1. **Hyperparameter Tuning**: Use Optuna or GridSearch for optimal parameters\n",
    "2. **Cross-Validation**: Implement K-fold CV for more robust evaluation\n",
    "3. **Feature Engineering**: Create domain-specific features (e.g., curve statistics)\n",
    "4. **Model Ensemble**: Try CatBoost, LightGBM in addition to XGBoost\n",
    "5. **Threshold Optimization**: Find optimal decision threshold for F1 or business metric\n",
    "6. **Production Deployment**: Package models for inference API\n",
    "\n",
    "### Manufacturing AI Best Practices Implemented:\n",
    "- Separate handling of design vs simulation data (domain knowledge)\n",
    "- Feature selection to reduce noise from high-dimensional data\n",
    "- Ensemble methods for critical quality decisions\n",
    "- Model tracking and versioning\n",
    "- Reproducible pipeline with random seeds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
