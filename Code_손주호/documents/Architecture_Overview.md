# Tire Defect Prediction - Hybrid Model Architecture

## ğŸ“Š í”„ë¡œì íŠ¸ ê°œìš”

**ëª©ì **: íƒ€ì´ì–´ ì„¤ê³„ ë° ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ë¥¼ í™œìš©í•œ ë¶ˆëŸ‰(Defect) ì˜ˆì¸¡
**ë¬¸ì œ ìœ í˜•**: Binary Classification (Good vs NG)
**ì ‘ê·¼ ë°©ë²•**: Hybrid Stacking Ensemble (Boosting + Deep Learning)

---

## ğŸ—ï¸ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Input Data (train.csv)                    â”‚
â”‚  - Design Features: Mass_Pilot, Width, Aspect, Proc_Param1~11  â”‚
â”‚  - Simulation Features: x0~x255, y0~y255, p0~p255, G1~G4       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Step 1: Feature Engineering Pipeline                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  Missing Val â”‚â†’ â”‚  Categorical â”‚â†’ â”‚   Feature    â”‚         â”‚
â”‚  â”‚   Handling   â”‚  â”‚   Encoding   â”‚  â”‚   Scaling    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Step 2: Feature Selection (XGBoost Importance)          â”‚
â”‚  All Features â†’ Top N Important Features (Default: Top 100)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                               â”‚
         â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Branch 1         â”‚          â”‚   Branch 2         â”‚
â”‚   XGBoost Model    â”‚          â”‚   DNN Model        â”‚
â”‚  (Design Features) â”‚          â”‚ (Simulation Feats) â”‚
â”‚                    â”‚          â”‚                    â”‚
â”‚  Output:           â”‚          â”‚  Output:           â”‚
â”‚  â€¢ p1: Probability â”‚          â”‚  â€¢ h2: Latent (64) â”‚
â”‚  â€¢ h1: Leaf Index  â”‚          â”‚  â€¢ p2: Prediction  â”‚
â”‚     (300 trees)    â”‚          â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Fusion Model (MLP)    â”‚
          â”‚                        â”‚
          â”‚  Input: [p1, h1, h2,   â”‚
          â”‚         p2]            â”‚
          â”‚                        â”‚
          â”‚  Layers:               â”‚
          â”‚  â€¢ FC(total_dim â†’ 64)  â”‚
          â”‚  â€¢ FC(64 â†’ 32)         â”‚
          â”‚  â€¢ FC(32 â†’ 1)          â”‚
          â”‚                        â”‚
          â”‚  Output: Final Defect  â”‚
          â”‚          Probability   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ ê° ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ì„¤ëª…

### 1ï¸âƒ£ **Feature Engineering Pipeline**

#### 1.1 FeaturePreprocessor Class

**ì—­í• **: ëª¨ë“ˆí™”ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥

**ì£¼ìš” ë©”ì„œë“œ**:
- `handle_missing_values()`: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (median/mean imputation)
- `encode_categorical()`: ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (LabelEncoder)
- `scale_features()`: ê·¸ë£¹ë³„ í‘œì¤€í™” (StandardScaler)
- `fit_transform()`: í•™ìŠµ ë°ì´í„°ì— fit + transform
- `transform()`: ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— transformë§Œ ì ìš©

**íŠ¹ì§•**:
- Verbose ëª¨ë“œë¡œ ê° ë‹¨ê³„ë³„ ì§„í–‰ìƒí™© ì¶œë ¥
- Feature ê·¸ë£¹ë³„(Design/Simulation) ë…ë¦½ì ì¸ ìŠ¤ì¼€ì¼ë§
- ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì²´ ì§€í–¥ êµ¬ì¡°

```python
# ì‚¬ìš© ì˜ˆì‹œ
preprocessor = FeaturePreprocessor(verbose=True)
df_processed = preprocessor.fit_transform(df_train,
                                          design_features,
                                          simulation_features,
                                          target_col)
```

---

### 2ï¸âƒ£ **Feature Selection**

#### 2.1 XGBoost ê¸°ë°˜ Feature Importance

**ëª©ì **: ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ íŠ¹ì§•ë§Œ ì„ íƒí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ë° ê³¼ì í•© ë°©ì§€

**í”„ë¡œì„¸ìŠ¤**:
1. ì „ì²´ íŠ¹ì§•ìœ¼ë¡œ ì„ì‹œ XGBoost ëª¨ë¸ í•™ìŠµ
2. `feature_importances_` ì¶”ì¶œ
3. ìƒìœ„ Nê°œ ë˜ëŠ” ì„ê³„ê°’ ì´ìƒ íŠ¹ì§• ì„ íƒ
4. Design íŠ¹ì§•ê³¼ Simulation íŠ¹ì§•ìœ¼ë¡œ ë¶„ë¦¬

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```python
TOP_N_FEATURES = 100  # ìƒìœ„ 100ê°œ íŠ¹ì§• ì„ íƒ
IMPORTANCE_THRESHOLD = 0.001  # ëŒ€ì•ˆ: ì„ê³„ê°’ ê¸°ë°˜ ì„ íƒ
```

**ê²°ê³¼**:
- ì„ íƒëœ Design íŠ¹ì§• â†’ Branch 1 (XGBoost)
- ì„ íƒëœ Simulation íŠ¹ì§• â†’ Branch 2 (DNN)

---

### 3ï¸âƒ£ **Branch 1: XGBoost Model (Design Features)**

#### 3.1 ëª¨ë¸ êµ¬ì¡°

**ì…ë ¥**: ì„ íƒëœ Design/Process Parameters (e.g., Width, Aspect, Proc_Param1~11)

**XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```python
xgb.XGBClassifier(
    n_estimators=300,        # íŠ¸ë¦¬ ê°œìˆ˜
    max_depth=7,             # íŠ¸ë¦¬ ê¹Šì´
    learning_rate=0.05,      # í•™ìŠµë¥ 
    subsample=0.8,           # í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨
    colsample_bytree=0.8,    # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨
    tree_method='hist',      # íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ (ë¹ ë¦„)
    eval_metric='logloss'    # í‰ê°€ ì§€í‘œ
)
```

#### 3.2 ì¶œë ¥

1. **p1 (Prediction Probability)**
   - í¬ê¸°: `(n_samples, 1)`
   - XGBoostì˜ ë¶ˆëŸ‰ ì˜ˆì¸¡ í™•ë¥ 
   - `predict_proba()[:, 1]`ë¡œ ì¶”ì¶œ

2. **h1 (Latent Features - Leaf Indices)**
   - í¬ê¸°: `(n_samples, n_estimators)` = `(n_samples, 300)`
   - ê° íŠ¸ë¦¬ì—ì„œ ìƒ˜í”Œì´ ë„ë‹¬í•œ ë¦¬í”„ ë…¸ë“œ ì¸ë±ìŠ¤
   - `apply()` ë©”ì„œë“œë¡œ ì¶”ì¶œ
   - **ì˜ë¯¸**: ì˜ì‚¬ê²°ì • ê²½ë¡œë¥¼ ì¸ì½”ë”©í•œ ê³ ìˆ˜ì¤€ íŠ¹ì§•

#### 3.3 íŠ¹ì§•

- **ì¥ì **: ë²”ì£¼í˜•/ìˆ˜ì¹˜í˜• í˜¼í•© ë°ì´í„° ì²˜ë¦¬ ìš°ìˆ˜
- **í•´ì„ë ¥**: Feature importance ì œê³µ
- **ê°•ê±´ì„±**: Outlierì— ê°•í•¨

---

### 4ï¸âƒ£ **Branch 2: Deep Neural Network (Simulation Features)**

#### 4.1 ëª¨ë¸ êµ¬ì¡° (SimulationDNN)

**ì…ë ¥**: ì„ íƒëœ Simulation Features (e.g., x0~x255, y0~y255, p0~p255, G1~G4)

**ì•„í‚¤í…ì²˜**:

```python
SimulationDNN(
    input_dim=len(selected_simulation_features),
    latent_dim=64,
    dropout_rate=0.3
)
```

**ë ˆì´ì–´ êµ¬ì„±**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Layer (input_dim)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(input_dim â†’ 256)             â”‚
â”‚ BatchNorm1d(256)                    â”‚
â”‚ ReLU()                              â”‚
â”‚ Dropout(0.3)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(256 â†’ 128)                   â”‚
â”‚ BatchNorm1d(128)                    â”‚
â”‚ ReLU()                              â”‚
â”‚ Dropout(0.3)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(128 â†’ 64)    [Latent h2]    â”‚
â”‚ BatchNorm1d(64)                     â”‚
â”‚ ReLU()                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚                          â”‚
               â–¼                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ Prediction Head         â”‚              â”‚
â”‚ Linear(64 â†’ 32)         â”‚              â”‚
â”‚ ReLU()                  â”‚              â”‚
â”‚ Dropout(0.3)            â”‚              â”‚
â”‚ Linear(32 â†’ 1)  [p2]    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                         â”‚
                        h2 (Latent) â”€â”€â”€â”€â”€â”˜
```

#### 4.2 ì¶œë ¥

1. **h2 (Latent Features)**
   - í¬ê¸°: `(n_samples, 64)`
   - ì¸ì½”ë”ì˜ ì••ì¶•ëœ í‘œí˜„
   - **ì˜ë¯¸**: ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì˜ ê³ ì°¨ì› íŒ¨í„´ì„ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•

2. **p2 (Prediction Logits)**
   - í¬ê¸°: `(n_samples, 1)`
   - DNNì˜ ë¶ˆëŸ‰ ì˜ˆì¸¡ (ë¡œì§“)
   - Sigmoidë¥¼ ê±°ì³ í™•ë¥ ë¡œ ë³€í™˜ ê°€ëŠ¥

#### 4.3 í•™ìŠµ ì„¤ì •

```python
# Loss Function
criterion = nn.BCEWithLogitsLoss()

# Optimizer
optimizer = optim.Adam(model.parameters(),
                       lr=0.001,
                       weight_decay=1e-5)

# Learning Rate Scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', patience=5, factor=0.5
)

# Early Stopping
patience = 15
n_epochs = 100
```

#### 4.4 íŠ¹ì§•

- **ì¥ì **: ê³ ì°¨ì› ë²¡í„° ë°ì´í„°(í”„ë¡œíŒŒì¼, ê³¡ì„ ) ì²˜ë¦¬ ìš°ìˆ˜
- **ì •ê·œí™”**: Batch Normalization + Dropoutìœ¼ë¡œ ê³¼ì í•© ë°©ì§€
- **ì ì‘ì  í•™ìŠµ**: ReduceLROnPlateauë¡œ í•™ìŠµë¥  ìë™ ì¡°ì •

---

### 5ï¸âƒ£ **Fusion Model (Final Decision Layer)**

#### 5.1 ëª¨ë¸ êµ¬ì¡° (HybridFusionModel)

**ì…ë ¥**: Branch 1ê³¼ Branch 2ì˜ ëª¨ë“  ì¶œë ¥ ê²°í•©

- `p1`: XGBoost ì˜ˆì¸¡ í™•ë¥  `(n_samples, 1)`
- `h1`: XGBoost ë¦¬í”„ ì¸ë±ìŠ¤ `(n_samples, 300)` â†’ StandardScalerë¡œ ì •ê·œí™”
- `h2`: DNN ì ì¬ íŠ¹ì§• `(n_samples, 64)`
- `p2`: DNN ì˜ˆì¸¡ ë¡œì§“ `(n_samples, 1)`

**Total Input Dimension**: `1 + 300 + 64 + 1 = 366`

**ì•„í‚¤í…ì²˜**:

```python
HybridFusionModel(
    boosting_pred_dim=1,
    boosting_latent_dim=300,
    dnn_latent_dim=64,
    dropout_rate=0.3
)
```

**ë ˆì´ì–´ êµ¬ì„±**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Concatenate [p1, h1, h2, p2]            â”‚
â”‚ Input Dimension: 366                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(366 â†’ 64)                        â”‚
â”‚ BatchNorm1d(64)                         â”‚
â”‚ ReLU()                                  â”‚
â”‚ Dropout(0.3)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(64 â†’ 32)                         â”‚
â”‚ BatchNorm1d(32)                         â”‚
â”‚ ReLU()                                  â”‚
â”‚ Dropout(0.3)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Linear(32 â†’ 1)                          â”‚
â”‚ Output: Final Defect Probability        â”‚
â”‚ (Apply Sigmoid for probability)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5.2 í•™ìŠµ ì „ëµ

**2ë‹¨ê³„ í•™ìŠµ (Two-Stage Training)**:

1. **Stage 1**: Branch 1ê³¼ Branch 2 ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ
   - XGBoost: Design íŠ¹ì§•ì— ëŒ€í•´ í•™ìŠµ
   - DNN: Simulation íŠ¹ì§•ì— ëŒ€í•´ í•™ìŠµ

2. **Stage 2**: Fusion Model í•™ìŠµ
   - Branch 1, 2ì˜ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
   - Branch ëª¨ë¸ì€ ê³ ì • (frozen) ë˜ëŠ” ë¯¸ì„¸ì¡°ì • ê°€ëŠ¥
   - í˜„ì¬ êµ¬í˜„: Branch ëª¨ë¸ ê³ ì •, Fusionë§Œ í•™ìŠµ

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```python
n_epochs = 80
batch_size = 64
learning_rate = 0.001
patience = 15  # Early stopping
```

#### 5.3 ì¥ì 

âœ… **ì•™ìƒë¸” íš¨ê³¼**: ë‘ ê°œì˜ ì´ì§ˆì ì¸ ëª¨ë¸ ê²°í•©ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
âœ… **íŠ¹ì§• ìƒí˜¸ì‘ìš©**: ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì†ŒìŠ¤ì˜ ìƒí˜¸ì‘ìš© í•™ìŠµ
âœ… **ìœ ì—°ì„±**: ê° ë¸Œëœì¹˜ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥
âœ… **ê°•ê±´ì„±**: í•œ ë¸Œëœì¹˜ì˜ ì•½ì ì„ ë‹¤ë¥¸ ë¸Œëœì¹˜ê°€ ë³´ì™„

---

## ğŸ“ˆ ë°ì´í„° íë¦„ (Data Flow)

### í•™ìŠµ ë‹¨ê³„ (Training Phase)

```
Raw Data (train.csv)
    â”‚
    â”œâ”€â†’ Feature Engineering
    â”‚       â”‚
    â”‚       â”œâ”€â†’ Missing Value Handling
    â”‚       â”œâ”€â†’ Categorical Encoding
    â”‚       â””â”€â†’ Feature Scaling
    â”‚
    â”œâ”€â†’ Feature Selection (XGBoost Importance)
    â”‚       â”‚
    â”‚       â”œâ”€â†’ Selected Design Features
    â”‚       â””â”€â†’ Selected Simulation Features
    â”‚
    â”œâ”€â†’ Train/Validation Split (80/20)
    â”‚
    â”œâ”€â†’ Branch 1: XGBoost Training
    â”‚       â”‚
    â”‚       â”œâ”€â†’ Input: Design Features
    â”‚       â””â”€â†’ Output: p1, h1
    â”‚
    â”œâ”€â†’ Branch 2: DNN Training
    â”‚       â”‚
    â”‚       â”œâ”€â†’ Input: Simulation Features
    â”‚       â””â”€â†’ Output: h2, p2
    â”‚
    â””â”€â†’ Fusion Model Training
            â”‚
            â”œâ”€â†’ Input: [p1, h1, h2, p2]
            â””â”€â†’ Output: Final Prediction
```

### ì¶”ë¡  ë‹¨ê³„ (Inference Phase)

```
New Data (test.csv)
    â”‚
    â”œâ”€â†’ Apply Fitted Preprocessor
    â”‚       â”‚
    â”‚       â””â”€â†’ Transform (no fitting)
    â”‚
    â”œâ”€â†’ Extract Selected Features
    â”‚       â”‚
    â”‚       â”œâ”€â†’ Design Features
    â”‚       â””â”€â†’ Simulation Features
    â”‚
    â”œâ”€â†’ XGBoost Inference
    â”‚       â”‚
    â”‚       â””â”€â†’ p1, h1
    â”‚
    â”œâ”€â†’ DNN Inference
    â”‚       â”‚
    â”‚       â””â”€â†’ h2, p2
    â”‚
    â””â”€â†’ Fusion Model Inference
            â”‚
            â””â”€â†’ Final Defect Probability
```

---

## ğŸ¯ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

### í‰ê°€ ì§€í‘œ

1. **Accuracy**: ì „ì²´ ì •í™•ë„
2. **Precision**: ë¶ˆëŸ‰ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ë¶ˆëŸ‰ ë¹„ìœ¨
3. **Recall**: ì‹¤ì œ ë¶ˆëŸ‰ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
4. **F1-Score**: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· 
5. **ROC-AUC**: ë¶„ë¥˜ ì„ê³„ê°’ì— ë¬´ê´€í•œ ì„±ëŠ¥ ì§€í‘œ

### ëª¨ë¸ ë¹„êµ

ë…¸íŠ¸ë¶ì—ì„œëŠ” ë‹¤ìŒ 3ê°€ì§€ ëª¨ë¸ì„ ë¹„êµ:

1. **XGBoost Only** (Design Features)
2. **DNN Only** (Simulation Features)
3. **Hybrid Fusion** (Combined)

â†’ Hybrid ëª¨ë¸ì´ ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ìš°ìˆ˜í•œ ì„±ëŠ¥ ê¸°ëŒ€

---

## ğŸ’¾ ì €ì¥ëœ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸

í•™ìŠµ ì™„ë£Œ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì´ ì €ì¥ë¨:

| íŒŒì¼ëª… | ì„¤ëª… | ì‚¬ìš© ìš©ë„ |
|--------|------|-----------|
| `preprocessor.pkl` | FeaturePreprocessor ê°ì²´ | ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš© |
| `xgb_model.json` | XGBoost ëª¨ë¸ | Branch 1 ì¶”ë¡  |
| `scaler_xgb_latent.pkl` | XGBoost ì ì¬íŠ¹ì§• ìŠ¤ì¼€ì¼ëŸ¬ | h1 ì •ê·œí™” |
| `best_dnn_model.pth` | DNN ëª¨ë¸ ê°€ì¤‘ì¹˜ | Branch 2 ì¶”ë¡  |
| `best_fusion_model.pth` | Fusion ëª¨ë¸ ê°€ì¤‘ì¹˜ | ìµœì¢… ì¶”ë¡  |
| `feature_config.pkl` | ì„ íƒëœ íŠ¹ì§• ë¦¬ìŠ¤íŠ¸ | íŠ¹ì§• ì¶”ì¶œ |

---

## ğŸ”„ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‚¬ìš©ë²•

```python
# 1. ëª¨ë¸ ë° ì•„í‹°íŒ©íŠ¸ ë¡œë“œ
import pickle
import torch

with open('preprocessor.pkl', 'rb') as f:
    preprocessor = pickle.load(f)

xgb_model = xgb.XGBClassifier()
xgb_model.load_model('xgb_model.json')

dnn_model = SimulationDNN(input_dim, latent_dim=64)
dnn_model.load_state_dict(torch.load('best_dnn_model.pth'))

fusion_model = HybridFusionModel(...)
fusion_model.load_state_dict(torch.load('best_fusion_model.pth'))

with open('feature_config.pkl', 'rb') as f:
    feature_config = pickle.load(f)

# 2. ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡
df_test = pd.read_csv('test.csv')

predictions = predict_tire_defects(
    df_test,
    preprocessor,
    xgb_model,
    dnn_model,
    fusion_model,
    feature_config['selected_design_features'],
    feature_config['selected_simulation_features'],
    scaler_xgb_latent,
    device
)

# 3. ê²°ê³¼ ì €ì¥
submission = pd.DataFrame({
    'id': df_test['id'],
    'prediction': predictions.flatten()
})
submission.to_csv('submission.csv', index=False)
```

---

## ğŸš€ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

**XGBoost**:
- `n_estimators`: [100, 200, 300, 500]
- `max_depth`: [5, 7, 9, 11]
- `learning_rate`: [0.01, 0.05, 0.1]
- `subsample`: [0.7, 0.8, 0.9, 1.0]

**DNN**:
- `latent_dim`: [32, 64, 128]
- `dropout_rate`: [0.2, 0.3, 0.4]
- `learning_rate`: [0.0001, 0.001, 0.01]
- Hidden layer sizes: ì‹¤í—˜ì ìœ¼ë¡œ ì¡°ì •

**Fusion**:
- Layer sizes: [64, 32] vs [128, 64, 32]
- `dropout_rate`: [0.2, 0.3, 0.4, 0.5]

### 2. Feature Engineering ê°œì„ 

- **ë„ë©”ì¸ íŠ¹ì§• ìƒì„±**:
  - ê³¡ì„  í†µê³„ëŸ‰ (í‰ê· , í‘œì¤€í¸ì°¨, ìµœëŒ€/ìµœì†Œ, ê¸°ìš¸ê¸°)
  - x, y, p ê°„ì˜ ìƒí˜¸ì‘ìš© íŠ¹ì§•
  - Fourier transform ê³„ìˆ˜ (ì£¼íŒŒìˆ˜ ë„ë©”ì¸)

- **ì°¨ì› ì¶•ì†Œ**:
  - PCAë¡œ x0~x255 ì••ì¶•
  - Autoencoderë¡œ ê³ ì°¨ì› íŠ¹ì§• ì••ì¶•

### 3. ì•™ìƒë¸” í™•ì¥

- **ë‹¤ì–‘í•œ Boosting ëª¨ë¸**:
  - CatBoost (ë²”ì£¼í˜• íŠ¹ì§• ì²˜ë¦¬ ìš°ìˆ˜)
  - LightGBM (ë¹ ë¥¸ í•™ìŠµ ì†ë„)

- **Model Stacking**:
  - Level 1: XGBoost, CatBoost, LightGBM, DNN
  - Level 2: Logistic Regression / Simple MLP

### 4. Cross-Validation

í˜„ì¬: Single train/val split (80/20)
ê°œì„ : 5-Fold Stratified CVë¡œ ë” ê°•ê±´í•œ í‰ê°€

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = []

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    # ê° foldë§ˆë‹¤ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    ...
```

### 5. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬

ë§Œì•½ NG/Good ë¹„ìœ¨ì´ ë¶ˆê· í˜•í•˜ë‹¤ë©´:
- **Oversampling**: SMOTE
- **Undersampling**: Random undersampling
- **Class Weights**: `scale_pos_weight` (XGBoost), `pos_weight` (BCE Loss)

---

## ğŸ“š ì œì¡° AI ëª¨ë²” ì‚¬ë¡€ (Manufacturing AI Best Practices)

### âœ… êµ¬í˜„ëœ ì‚¬í•­

1. **ë„ë©”ì¸ ì§€ì‹ ë°˜ì˜**:
   - Design vs Simulation íŠ¹ì§• ë¶„ë¦¬
   - ê° íŠ¹ì§• ê·¸ë£¹ì— ì í•©í•œ ëª¨ë¸ ì„ íƒ

2. **ì¬í˜„ì„± (Reproducibility)**:
   - ëª¨ë“  random seed ê³ ì •
   - ë²„ì „ ê´€ë¦¬ ê°€ëŠ¥í•œ êµ¬ì¡°

3. **ëª¨ë“ˆí™” (Modularity)**:
   - ë…ë¦½ì ì¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
   - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸

4. **ì¶”ì ì„± (Traceability)**:
   - ëª¨ë“  ë‹¨ê³„ì—ì„œ verbose ë¡œê·¸
   - ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥

5. **ìƒì‚° ì¤€ë¹„ (Production-Ready)**:
   - End-to-end ì¶”ë¡  íŒŒì´í”„ë¼ì¸
   - ëª¨ë¸ ì§ë ¬í™” ë° ë¡œë”©

### ğŸ¯ ì¶”ê°€ ê¶Œì¥ì‚¬í•­

1. **ëª¨ë¸ ëª¨ë‹ˆí„°ë§**:
   - Prediction drift ê°ì§€
   - Feature distribution ë³€í™” ì¶”ì 

2. **A/B í…ŒìŠ¤íŒ…**:
   - ìƒˆ ëª¨ë¸ vs ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
   - ì ì§„ì  ë°°í¬

3. **ì„¤ëª… ê°€ëŠ¥ì„± (Explainability)**:
   - SHAP valuesë¡œ ì˜ˆì¸¡ ì„¤ëª…
   - Feature importance ì‹œê°í™”

4. **ë°ì´í„° ë²„ì „ ê´€ë¦¬**:
   - DVC (Data Version Control) ì‚¬ìš©
   - í•™ìŠµ ë°ì´í„° ìŠ¤ëƒ…ìƒ· ê´€ë¦¬

---

## ğŸ”— ì°¸ê³  ìë£Œ

- **XGBoost Documentation**: https://xgboost.readthedocs.io/
- **PyTorch Documentation**: https://pytorch.org/docs/
- **Scikit-learn User Guide**: https://scikit-learn.org/stable/user_guide.html
- **Manufacturing AI Papers**:
  - "Deep Learning for Smart Manufacturing" (2018)
  - "Hybrid Models for Quality Prediction in Manufacturing" (2020)

---

## ğŸ“ ë¬¸ì˜ ë° ì§€ì›

í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜:
- ì‘ì„±ì: ì†ì£¼í˜¸
- ë‚ ì§œ: 2025
- ìš©ë„: ë°ì´í„° ëŒ€íšŒ / íƒ€ì´ì–´ ë¶ˆëŸ‰ ì˜ˆì¸¡

---

**Document Version**: 1.0
**Last Updated**: 2025-12-06
**Architecture Type**: Hybrid Stacking Ensemble (XGBoost + DNN)
