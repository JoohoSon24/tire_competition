{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoohoSon24/tire_competition/blob/main/%EC%86%90%EC%A3%BC%ED%98%B8_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q2ICLckiMdhX",
        "outputId": "08b764fa-e2d4-4e63-dc42-a11a041c0b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3607358659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp drive/MyDrive/25F/Tire/5-ai-and-datascience-competition.zip .'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -d data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp drive/MyDrive/25F/Tire/5-ai-and-datascience-competition.zip .\n",
        "!mkdir -d data\n",
        "!unzip 5-ai-and-datascience-competition.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhg_Sjczv8vW"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx-F5a_yOagq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer, OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "tab10_kwargs = {'cmap': 'tab10', 'vmin': 0, 'vmax': 9}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHdKAfp_On1u"
      },
      "outputs": [],
      "source": [
        "def split_data(df, train=True):\n",
        "    cols_sim = [f'{ch}{i}' for i in range(256) for ch in ['x', 'y', 'p']]\n",
        "    X_sum = df.drop(columns=cols_sim)\n",
        "    X_fem = df[cols_sim]\n",
        "    if train:\n",
        "      X_sum = X_sum.drop(columns='Class')\n",
        "      y = df['Class']\n",
        "      return X_sum, X_fem, y\n",
        "    else:\n",
        "      X_sum = X_sum.drop(columns='ID')\n",
        "      ids = df['ID']\n",
        "      return X_sum, X_fem, ids\n",
        "\n",
        "def numerize(X_sum, oe=None):\n",
        "  cat_cols = X_sum.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
        "  num_cols = X_sum.select_dtypes(exclude=['object', 'category', 'bool']).columns.tolist()\n",
        "\n",
        "  train = oe is None\n",
        "\n",
        "  Xn = X_sum[num_cols].values\n",
        "  if train:\n",
        "    oe = OneHotEncoder(sparse_output=False)\n",
        "    Xc = oe.fit_transform(X_sum[cat_cols])\n",
        "  else:\n",
        "    Xc = oe.transform(X_sum[cat_cols])\n",
        "\n",
        "  ohe_cols = oe.get_feature_names_out(cat_cols).tolist()\n",
        "  X = np.concatenate([Xc, Xn], axis=1)\n",
        "  X = pd.DataFrame(data=X, columns=ohe_cols+num_cols, index=X_sum.index)\n",
        "  if train:\n",
        "    return X, oe\n",
        "  else:\n",
        "    return X\n",
        "\n",
        "def apply_smote(X, y, random_state=42):\n",
        "    smote = SMOTE(random_state=random_state)\n",
        "    X_res, y_res = smote.fit_resample(X, y)\n",
        "    return X_res, y_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hw27EFDHPB8"
      },
      "outputs": [],
      "source": [
        "def curve_length(x, y, p):\n",
        "    P = np.stack([x, y], axis=-1)\n",
        "    return np.sqrt((np.diff(P, axis=-2) ** 2).sum(axis=-1)).sum(axis=-1)\n",
        "\n",
        "def stress_length(x, y, p, p_thr=2.5):\n",
        "    P = np.stack([x, y], axis=-1)  # (N, l, 2)\n",
        "    mask = (p > p_thr)  # (N, l)\n",
        "\n",
        "    ls = []\n",
        "    for i in range(p.shape[0]):\n",
        "        P_stress = P[i, mask[i], :]  # (l', 2)\n",
        "        l = np.sqrt((np.diff(P_stress, axis=0) ** 2).sum(axis=-1)).sum(axis=-1)\n",
        "        ls.append(l)\n",
        "    ls = np.array(ls)\n",
        "\n",
        "    return ls\n",
        "\n",
        "def bend_extent(x, y, p):\n",
        "    return (y.max(axis=-1) - y.min(axis=-1)) / (x.max(axis=-1) - x.min(axis=-1))\n",
        "\n",
        "def max_curvature(x, y, p):\n",
        "    dx  = np.gradient(x, axis=1)\n",
        "    dy  = np.gradient(y, axis=1)\n",
        "    ddx = np.gradient(dx, axis=1)\n",
        "    ddy = np.gradient(dy, axis=1)\n",
        "\n",
        "    num = np.abs(dx * ddy - dy * ddx)\n",
        "    den = (dx**2 + dy**2)**1.5 + 1e-9\n",
        "\n",
        "    curvature = num / den\n",
        "    max_kappa = np.max(curvature, axis=1)\n",
        "    return max_kappa\n",
        "\n",
        "def FEM_feat(rows, fn=lambda x, y, p: p.max(axis=-1)):\n",
        "    if isinstance(rows, (pd.Series, pd.DataFrame)):\n",
        "        rows = rows.values\n",
        "    x, y, p = np.transpose(rows.reshape(-1, rows.shape[1] // 3, 3), axes=(2, 0, 1))\n",
        "    return fn(x, y, p)\n",
        "\n",
        "def extract_fem(X_fem, fns=lambda x, y, p: p.max(axis=-1), as_df=False):\n",
        "  if isinstance(X_fem, (pd.Series, pd.DataFrame)):\n",
        "    rows = X_fem.values\n",
        "  else:\n",
        "    rows = X_fem\n",
        "\n",
        "  if not isinstance(fns, list):\n",
        "    fns = [fns]\n",
        "\n",
        "  X_fem_new = []\n",
        "  for fn in fns:\n",
        "    feat = FEM_feat(X_fem, fn=fn)\n",
        "    X_fem_new.append(feat)\n",
        "  X_fem_new = np.stack(X_fem_new, axis=1)\n",
        "\n",
        "  if as_df:\n",
        "    X_fem_new = pd.DataFrame(data = X_fem_new, columns=[f'FEM_feat_{i}' for i in range(len(fns))])\n",
        "\n",
        "  return X_fem_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6RHjqqX7Bj3"
      },
      "outputs": [],
      "source": [
        "def conf_table(y_pred, y_true):\n",
        "    if y_true.ndim == 2:\n",
        "      y_true = y_true.squeeze(1)\n",
        "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
        "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
        "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
        "    tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
        "    return tp, tn, fp, fn\n",
        "\n",
        "def plot_curves(p_hat, y_true, ax, curve_type='auroc', n=100):\n",
        "    if y_true.ndim == 2:\n",
        "      y_true = y_true.squeeze(1)\n",
        "\n",
        "    x, y = [], []\n",
        "    for thr in np.linspace(0, 1, n + 1):\n",
        "      y_pred = (p_hat > thr).astype(int)\n",
        "\n",
        "      tp, tn, fp, fn = conf_table(y_pred, y_true)\n",
        "      if curve_type == 'auroc':\n",
        "        xx = fp / np.clip((tn + fp), 1, np.inf) # FPR\n",
        "        yy = tp / np.clip((tp + fn), 1, np.inf) # Recall, TPR\n",
        "      elif curve_type == 'auprc':\n",
        "        xx = tp / np.clip((tp + fn), 1, np.inf) # Recall, TPR\n",
        "        yy = tp / (tp + fp) if tp + fp > 0 else 1 # Precision\n",
        "      elif curve_type == 'NPV':\n",
        "        xx = thr\n",
        "        yy = tn / (tn + fn) if tn + fn > 0 else 1\n",
        "      elif curve_type == 'profit':\n",
        "        xx = thr\n",
        "        yy = tn * 100 - fn * 2000 - 99999 * int(fn + tn > 200 * len(y_true) / 466)\n",
        "      x.append(xx)\n",
        "      y.append(yy)\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    if curve_type in ['auroc', 'auprc']:\n",
        "      score = -((y[1:] + y[:-1]) / 2 * np.diff(x)).sum()\n",
        "      ax.plot([0, 1], [0, 1] if curve_type == 'auroc' else [1, 0], 'k--', alpha=0.3)\n",
        "      xlabel = 'FPR' if curve_type == 'auroc' else 'Recall'\n",
        "      ylabel = 'TPR' if curve_type == 'auroc' else 'Precision'\n",
        "      print(f\"{curve_type.upper()} score: {score:.4f}\")\n",
        "    elif curve_type == 'NPV':\n",
        "      score = y[len(y)//2]\n",
        "      print(f\"NPV at thr=0.5: {score:.4f}\")\n",
        "      xlabel = 'Threshold'\n",
        "      ylabel = 'NPV'\n",
        "    elif curve_type == 'profit':\n",
        "      thr_opt = x[np.argmax(y)]\n",
        "      score = np.max(y)\n",
        "      print(f\"Actual Optimal Profit: {score:.1f} at thr={thr_opt:.4f}\")\n",
        "      xlabel = 'Threshold'\n",
        "      ylabel = 'Profit'\n",
        "\n",
        "    ax.plot(x, y, c='red', lw=1)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(f'{curve_type} curve)')\n",
        "\n",
        "    return score\n",
        "\n",
        "def plot_profit(p_hat, y_true, ax, quantile=[0.25, 0.75], n=100, xlim=None, ylim=None):\n",
        "    x, y_expected_mean, y_expected_std = [], [], []\n",
        "    if y_true is None:\n",
        "      y_true = np.zeros_like(p_hat)\n",
        "    for thr in np.linspace(0, 1, n + 1):\n",
        "      y_pred = (p_hat > thr).astype(int)\n",
        "      p_selected = p_hat[p_hat < thr]\n",
        "\n",
        "      y_bar = (100 * (1 - p_selected) - 2000 * p_selected).sum() - 99999 * int(len(p_selected) > 200 * len(p_hat) / 466)\n",
        "      y_std = 2100 * np.sqrt((p_selected * (1 - p_selected)).sum())\n",
        "\n",
        "      x.append(thr)\n",
        "      y_expected_mean.append(y_bar)\n",
        "      y_expected_std.append(y_std)\n",
        "\n",
        "    x = np.array(x)\n",
        "    y_expected_mean = np.array(y_expected_mean)\n",
        "    y_expected_std = np.array(y_expected_std)\n",
        "\n",
        "    #thr_opt_expected = x[np.argmax(y_expected_mean)]\n",
        "    thr_opt_expected = 0.03\n",
        "    p_max_expected = np.max(y_expected_mean)\n",
        "    p_std_expected = y_expected_std[np.argmax(y_expected_mean)]\n",
        "\n",
        "    y_pred = (p_hat > thr_opt_expected).astype(int)\n",
        "    tp, tn, fp, fn = conf_table(y_pred, y_true)\n",
        "    p_decision = tn * 100 - fn * 2000 - 99999 * int(fn + tn > 200 * len(y_true) / 466)\n",
        "\n",
        "    print(f\"Expected Optimal Profit: {p_max_expected:.1f}(±{p_std_expected:.1f}) at thr={thr_opt_expected:.4f}\")\n",
        "    print(f\"Decision Profit: {p_decision:.1f} at thr={thr_opt_expected:.4f}\")\n",
        "\n",
        "    ax.plot(x, y_expected_mean, c='black', lw=1)\n",
        "    y_q0 = y_q = y_expected_mean + norm.ppf(quantile[0]) * y_expected_std\n",
        "    for q in quantile:\n",
        "      y_q = y_expected_mean + norm.ppf(q) * y_expected_std\n",
        "      ax.plot(x, y_q, c='grey', label=f\"{q*100}%\", lw=1)\n",
        "    ax.fill_between(x, y_q0, y_q, fc='gray', alpha=0.3)\n",
        "\n",
        "    if xlim is not None:\n",
        "      ax.set_xlim(xlim)\n",
        "    if ylim is not None:\n",
        "      ax.set_ylim(ylim)\n",
        "\n",
        "    ax.set_xlabel('Decision Threshold')\n",
        "    ax.set_ylabel('Profit')\n",
        "    ax.set_title(f'Profit curve')\n",
        "    plt.legend()\n",
        "\n",
        "    return thr_opt_expected, p_max_expected, p_std_expected, p_decision\n",
        "\n",
        "def print_result(model, X, y_true):\n",
        "    y_pred = model.predict(X)\n",
        "    tp, tn, fp, fn = conf_table(y_pred, y_true)\n",
        "\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "    recall = (tp) / (tp + fn)\n",
        "    precision = (tp) / (tp + fp)\n",
        "    npv = (tn) / (tn + fn)\n",
        "    f1 = 2 * (recall * precision) / (recall + precision)\n",
        "\n",
        "    print(f\"Ground truth positive rate: {y_true.mean():.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"NPV: {npv:.4f}\")\n",
        "    print(f\"F1 score: {f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "def final_score(auroc, profit_mean, profit_std):\n",
        "  return np.sqrt(max(auroc - 0.5, 0) / 0.5 * max(profit_mean, 0) / 20000).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aLnBa9ymPwnD"
      },
      "outputs": [],
      "source": [
        "def plot_FEM(row, ax):\n",
        "    if isinstance(row, pd.Series):\n",
        "        row = row.values\n",
        "    x, y, p = row.reshape(-1, 3).T\n",
        "    im = ax.scatter(x, y, c=p, cmap='jet', s=0.5, vmin=1.2, vmax=3)\n",
        "    plt.colorbar(im)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# 1. 'data' 폴더 생성 (이미 있으면 무시)\n",
        "# 주의: '/data'는 시스템 루트 경로이므로 권한 문제가 생길 수 있습니다. 현재 위치 기준인 'data'를 추천합니다.\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "print(\"아래 [파일 선택] 버튼을 눌러 train.csv와 test.csv를 모두 선택해 주세요.\")\n",
        "\n",
        "# 2. 로컬 파일 업로드 (train.csv, test.csv 선택)\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 3. 업로드된 파일을 'data' 폴더로 이동\n",
        "for filename in uploaded.keys():\n",
        "    # 파일이 이미 data 폴더에 있다면 덮어쓰기 위해 이동 전 확인\n",
        "    source = filename\n",
        "    destination = os.path.join('data', filename)\n",
        "\n",
        "    # 이동 (shutil.move는 파일 이동 명령어)\n",
        "    shutil.move(source, destination)\n",
        "    print(f\"✅ {filename} 파일이 {destination} 위치로 이동되었습니다.\")"
      ],
      "metadata": {
        "id": "5OPvU64_f9B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_tSF2-5Oe26"
      },
      "outputs": [],
      "source": [
        "df_pub = pd.read_csv('data/train.csv')\n",
        "df_exam = pd.read_csv('data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7F-Lc78tQkH"
      },
      "outputs": [],
      "source": [
        "X_sum, X_fem, cl_ = split_data(df_pub)\n",
        "X_sum_exam, X_fem_exam, ids = split_data(df_exam, train=False)\n",
        "\n",
        "le = LabelBinarizer()\n",
        "cl = le.fit_transform(cl_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-HDzRh6QMKj"
      },
      "outputs": [],
      "source": [
        "def cross_validate(model, X, y, cv=5, seed=None):\n",
        "  if isinstance(y, pd.Series):\n",
        "    y = y.values\n",
        "  if y.ndim == 2:\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "  N = X.shape[0]\n",
        "  index = np.random.permutation(np.arange(N))\n",
        "  X, y = X.iloc[index], y[index]\n",
        "\n",
        "  block_size = N // cv\n",
        "  ps, ys = [], []\n",
        "  for i in range(cv):\n",
        "    s, e = block_size * i, min(block_size * (i + 1), N)\n",
        "    mask = ((np.arange(N) >= s) & (np.arange(N) < e))\n",
        "    X_train, X_val = X.iloc[~mask], X.iloc[mask]\n",
        "    y_train, y_val = y[~mask], y[mask]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    p = model.predict_proba(X_val)[:, 1]\n",
        "    ps.append(p)\n",
        "    ys.append(y_val)\n",
        "  ps = np.stack(ps)\n",
        "  ys = np.stack(ys)\n",
        "\n",
        "  return ps, ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrjl-eeSi95M"
      },
      "source": [
        "# 4. FEM Info. Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ4JacH8kegw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def collate_fn(batch):\n",
        "  keys = batch[0][-1].keys()\n",
        "  xs = torch.stack([x for x, y, p, l in batch])\n",
        "  ys = torch.stack([y for x, y, p, l in batch])\n",
        "  ps = torch.stack([p for x, y, p, l in batch])\n",
        "  labels = {k: torch.tensor([l[k] for x, y, p, l in batch]).to(torch.float32) for k in keys}\n",
        "  return xs, ys, ps, labels\n",
        "\n",
        "class FEMDataset(Dataset):\n",
        "  def __init__(self, df, labels):\n",
        "    self.df = df\n",
        "    labels = (labels - labels.mean()) / labels.std()\n",
        "    self.labels = {col: labels[col].values for col in labels.columns}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    row = self.df.iloc[i].values\n",
        "    x, y, p = row.reshape(-1, 3).T\n",
        "    x = torch.from_numpy(x).to(torch.float32)\n",
        "    y = torch.from_numpy(y).to(torch.float32)\n",
        "    p = torch.from_numpy(p).to(torch.float32)\n",
        "\n",
        "    x = (x - x[0]) / 100\n",
        "    y = (y - y[0]) / 20\n",
        "\n",
        "    l = {k: v[i] for k, v in self.labels.items()}\n",
        "    return x, y, p, l\n",
        "\n",
        "  def get_labels(self):\n",
        "    return self.labels\n",
        "\n",
        "class FEMEncoder(nn.Module):\n",
        "  def __init__(self, in_dim=256*3, hid_dim=256, rep_dim=16, p=0.1, targets: list = ['G1', 'G2', 'G3']):\n",
        "    super().__init__()\n",
        "    self.body = nn.Sequential(\n",
        "        nn.Linear(in_dim, hid_dim),\n",
        "        nn.Dropout(p=p),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hid_dim, rep_dim),\n",
        "        nn.Dropout(p=p),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    heads = {}\n",
        "    for k in targets:\n",
        "      heads[k] = nn.Linear(rep_dim, 1)\n",
        "    self.heads = nn.ModuleDict(heads)\n",
        "    self.targets = list(self.heads.keys())\n",
        "\n",
        "  def get_repv(self, x, y, p):\n",
        "    h = torch.cat([x, y, p], dim=-1)\n",
        "    return self.body(h)\n",
        "\n",
        "  def forward(self, x, y, p):\n",
        "    h = self.get_repv(x, y, p)\n",
        "    ys = {}\n",
        "    for k in self.targets:\n",
        "      ys[k] = self.heads[k](h).squeeze(-1)\n",
        "    return ys\n",
        "\n",
        "class CNNEncoder(FEMEncoder):\n",
        "    def __init__(self, in_dim=3, hid_dim=64, rep_dim=16, n_blocks=4, p=.1, targets: list = ['G1', 'G2', 'G3']):\n",
        "        \"\"\"\n",
        "        channels: 입력 채널 수 (x, y, p → 3)\n",
        "        hid_dim: Conv hidden size\n",
        "        layers: dilated residual block 개수\n",
        "        \"\"\"\n",
        "        super().__init__(in_dim, hid_dim, rep_dim, p, targets)\n",
        "        del self.body\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        in_channels = in_dim\n",
        "        out_channels = hid_dim\n",
        "        dilation = 1\n",
        "        for _ in range(n_blocks):\n",
        "            padding = dilation\n",
        "            block = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
        "                          dilation=dilation, padding=padding),\n",
        "                nn.Dropout(p=p),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "            self.convs.append(block)\n",
        "            in_channels = out_channels\n",
        "            out_channels = hid_dim\n",
        "            dilation *= 2\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hid_dim, hid_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hid_dim, rep_dim)\n",
        "        )\n",
        "\n",
        "    def get_repv(self, x, y, p):\n",
        "        \"\"\"\n",
        "        x, y, p: 각각 shape (batch, T)\n",
        "        return: z (batch, hidden_dim)\n",
        "        \"\"\"\n",
        "        # (B, T) → (B, 1, T)\n",
        "        x = x.unsqueeze(1)\n",
        "        y = y.unsqueeze(1)\n",
        "        p = p.unsqueeze(1)\n",
        "\n",
        "        # concat → (B, 3, T)\n",
        "        h = torch.cat([x, y, p], dim=1)\n",
        "\n",
        "        # CNN forward\n",
        "        for conv in self.convs:\n",
        "            h = conv(h)\n",
        "\n",
        "        z = h.mean(dim=-1)\n",
        "\n",
        "        # MLP → final representation\n",
        "        z = self.mlp(z)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpn569URl5T_"
      },
      "outputs": [],
      "source": [
        "def loss_medusa(preds, trues, weight=None):\n",
        "  assert preds.keys() == trues.keys()\n",
        "\n",
        "  loss = torch.zeros(1, device=list(preds.values())[0].device)\n",
        "  for k in trues.keys():\n",
        "    pred, true = preds[k], trues[k]\n",
        "    assert pred.device == true.device, f\"pred: {pred.device} / true: {true.device}\"\n",
        "    loss += nn.functional.mse_loss(pred, true)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p0N5bgWpyUD"
      },
      "outputs": [],
      "source": [
        "X_fem_all = pd.concat([X_fem, X_fem_exam], axis=0).reset_index(drop=True)\n",
        "X_sum_all = pd.concat([X_sum, X_sum_exam], axis=0).reset_index(drop=True)\n",
        "\n",
        "N = len(X_fem_all)\n",
        "r = .8\n",
        "max_epoch = 100\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "target_cols = ['Y1', 'G1', 'X1', 'Y5', 'X5', 'Y2', 'G2', 'G4', 'G3', 'Y3']#'Proc_Param8'] # Y1, G1, X1, Y5, X5\n",
        "loss_fn = loss_medusa\n",
        "\n",
        "indices = torch.randperm(N)\n",
        "\n",
        "train_set = FEMDataset(X_fem_all.iloc[indices[:int(N*r)]], labels=X_sum_all[target_cols].iloc[indices[:int(N*r)]])\n",
        "val_set = FEMDataset(X_fem_all.iloc[indices[int(N*r):]], labels=X_sum_all[target_cols].iloc[indices[int(N*r):]])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=16, collate_fn=collate_fn, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lITvBnEE-7oD"
      },
      "outputs": [],
      "source": [
        "model_fem = CNNEncoder(hid_dim=256, targets=target_cols)\n",
        "for batch in val_loader:\n",
        "  x, y, p, ls = batch\n",
        "  ls_pred = model_fem(x, y, p)\n",
        "  print({k: v.shape for k, v in ls_pred.items()}); break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6r3xlk3EqnfH"
      },
      "outputs": [],
      "source": [
        "model_fem = CNNEncoder(hid_dim=128, rep_dim=12, p=0.1, targets=target_cols)\n",
        "optimizer = Adam(params=model_fem.parameters(), lr=1e-3, weight_decay=2e-4)\n",
        "\n",
        "def run_epoch(train, epoch, device, verbose=True):\n",
        "  loader = train_loader if train else val_loader\n",
        "  epoch_loss = []\n",
        "  bar = tqdm(loader) if verbose else loader\n",
        "  for batch in bar:\n",
        "    x, y, p, labels = batch\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    p = p.to(device)\n",
        "    for k in labels.keys():\n",
        "      labels[k] = labels[k].to(device)\n",
        "\n",
        "    labels_pred = model_fem(x, y, p)\n",
        "    loss = loss_fn(labels_pred, labels)\n",
        "    if train:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    epoch_loss.append(loss.item())\n",
        "  epoch_loss = sum(epoch_loss) / len(epoch_loss)\n",
        "  return epoch_loss\n",
        "\n",
        "model_size = sum([param.numel() for param in model_fem.parameters()])\n",
        "print(f\"Model size: {model_size}\")\n",
        "\n",
        "loss_dict = {'train': [], 'val': []}\n",
        "model_fem.to(device)\n",
        "for i in range(max_epoch):\n",
        "  verbose = (i % 10 == 0)\n",
        "  model_fem.train()\n",
        "  train_loss = run_epoch(train=True, epoch=i, device=device, verbose=verbose)\n",
        "  loss_dict['train'].append(train_loss)\n",
        "  print(f\"Epoch {i} training loss: {train_loss:.6f}\") if verbose else None\n",
        "\n",
        "  model_fem.eval()\n",
        "  with torch.no_grad():\n",
        "    val_loss = run_epoch(train=False, epoch=i, device=device, verbose=verbose)\n",
        "  loss_dict['val'].append(val_loss)\n",
        "  print(f\"Epoch {i} validation loss: {val_loss:.6f}\") if verbose else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwWx1xfwsNkP"
      },
      "outputs": [],
      "source": [
        "labels = val_set.get_labels()\n",
        "labels_pred = {k: [] for k in target_cols}\n",
        "model_fem.eval()\n",
        "for batch in val_loader:\n",
        "  x, y, p, _ = batch\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  p = p.to(device)\n",
        "  with torch.no_grad():\n",
        "    out = model_fem(x, y, p)\n",
        "    for k in target_cols:\n",
        "      labels_pred[k].append(out[k])\n",
        "\n",
        "for k in target_cols:\n",
        "  labels_pred[k] = torch.cat(labels_pred[k]).detach().cpu().numpy()\n",
        "\n",
        "r, c = 6, 2\n",
        "fig, axes = plt.subplots(r, c, figsize=(6 * c, 4 * r))\n",
        "\n",
        "axes[0, 0].plot(loss_dict['train'], label='train')\n",
        "axes[0, 0].plot(loss_dict['val'], label='val')\n",
        "axes[0, 0].set_title(\"Loss plot\")\n",
        "axes[0, 0].legend()\n",
        "\n",
        "for n, k in enumerate(target_cols):\n",
        "  i, j = (n + 1) // c, (n + 1) % c\n",
        "  ax = axes[i, j]\n",
        "  label, label_pred = labels[k], labels_pred[k]\n",
        "  m, M = np.sort(np.concatenate([label, label_pred]))[[0, -1]]\n",
        "\n",
        "  ax.scatter(label, label_pred, s=5)\n",
        "  ax.plot([m, M], [m, M], lw=0.5, c='black')\n",
        "  ax.set_xlim(m - (M - m) * 0.05, M + (M - m) * 0.05)\n",
        "  ax.set_ylim(m - (M - m) * 0.05, M + (M - m) * 0.05)\n",
        "\n",
        "  SST = ((label - label.mean()) ** 2).sum()\n",
        "  SSE = ((label_pred - label) ** 2).sum()\n",
        "  R_sq =  1 - SSE / SST\n",
        "  ax.set_title(f\"Feature {k} - R sq.: {R_sq:4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GR6eMCJsva-"
      },
      "outputs": [],
      "source": [
        "pub_set = FEMDataset(X_fem, labels=X_sum[target_cols])\n",
        "pub_loader = DataLoader(pub_set, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "rep_vs = []\n",
        "model_fem.eval()\n",
        "for batch in pub_loader:\n",
        "  x, y, p, _ = batch\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  p = p.to(device)\n",
        "  with torch.no_grad():\n",
        "    rep_vs.append(model_fem.get_repv(x, y, p))\n",
        "rep_vs = torch.cat(rep_vs, dim=0).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUl0BbftpVUy"
      },
      "outputs": [],
      "source": [
        "df_fem_feat = pd.DataFrame(\n",
        "    data = rep_vs,\n",
        "    columns = [f'FEM_feat_{i}' for i in range(rep_vs.shape[1])]\n",
        ")\n",
        "df_fem_feat.to_csv('data/fem_ext.csv')\n",
        "df_fem_feat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_a80fZ5vqEz"
      },
      "source": [
        "# 1. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0qQcpMZvo3Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier, XGBRFClassifier\n",
        "\n",
        "import shap\n",
        "\n",
        "model_type = 'rf' # 'catboost' 'xgb' 'rf'\n",
        "include_fem = True\n",
        "augment = False\n",
        "\n",
        "if model_type == 'catboost':\n",
        "  model = CatBoostClassifier(\n",
        "      cat_features = ['Plant', 'Proc_Param6'],\n",
        "      verbose = 0,\n",
        "      )\n",
        "  X = X_sum\n",
        "\n",
        "elif model_type == 'xgb':\n",
        "  model = XGBRFClassifier(\n",
        "      n_estimators = 1000,\n",
        "      scale_pos_weight=1,\n",
        "      )\n",
        "  X, oe = numerize(X_sum)\n",
        "\n",
        "elif model_type == 'rf':\n",
        "  model = RandomForestClassifier(\n",
        "      n_estimators = 1000,\n",
        "      ##########################################class_weight={0: 1, 1: 20,\n",
        "      max_features = \"sqrt\",\n",
        "      #max_depth = 25,\n",
        "      #min_samples_split = 50,\n",
        "      #min_samples_leaf = 16,\n",
        "      bootstrap = True,\n",
        "      criterion = \"log_loss\"\n",
        "      )\n",
        "  X, oe = numerize(X_sum)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def engineer_specs(X):\n",
        "    \"\"\"\n",
        "    1. 타이어 제원(Spec) 기반 물리량 계산\n",
        "    - 단순 Width, Aspect 등이 아닌 실제 물리적 높이, 지름, 부피 등을 유도\n",
        "    \"\"\"\n",
        "    df = X.copy()\n",
        "\n",
        "    # Mass_Pilot: Boolean -> Int (1/0)\n",
        "    if 'Mass_Pilot' in df.columns:\n",
        "        df['Mass_Pilot'] = df['Mass_Pilot'].astype(int)\n",
        "\n",
        "    # Plant: One-Hot Encoding (공장이 여러 곳일 경우를 대비)\n",
        "    if 'Plant' in df.columns:\n",
        "        df = pd.get_dummies(df, columns=['Plant'], prefix='Plant')\n",
        "\n",
        "    # 물리 공식 적용\n",
        "    # 1. Sidewall Height (단면 높이 mm) = 단면폭 * 편평비 / 100\n",
        "    df['Feat_Sidewall_H'] = df['Width'] * (df['Aspect'] / 100)\n",
        "\n",
        "    # 2. Total Diameter (타이어 전체 지름 mm) = (단면 높이 * 2) + (휠 인치 * 25.4)\n",
        "    df['Feat_Diameter'] = (df['Feat_Sidewall_H'] * 2) + (df['Inch'] * 25.4)\n",
        "\n",
        "    # 3. Tire Volume Proxy (타이어 부피 근사치) = 지름 * 단면폭 (적재/물류 효율성 관련)\n",
        "    df['Feat_Volume_Proxy'] = df['Feat_Diameter'] * df['Width']\n",
        "\n",
        "    # 4. Aspect Ratio Interaction (인치 대비 폭 비율)\n",
        "    df['Feat_Width_to_Inch'] = df['Width'] / df['Inch']\n",
        "\n",
        "    return df\n",
        "\n",
        "def engineer_geometry(X):\n",
        "    \"\"\"\n",
        "    2. 형상 벡터(X1~X5, Y1~Y5) 기하학적 처리\n",
        "    - 단순 좌표가 아닌 다각형의 면적, 둘레, 왜도 등을 계산\n",
        "    \"\"\"\n",
        "    df = X.copy()\n",
        "\n",
        "    # 좌표 컬럼 존재 여부 확인\n",
        "    x_cols = [f'X{i}' for i in range(1, 6)]\n",
        "    y_cols = [f'Y{i}' for i in range(1, 6)]\n",
        "\n",
        "    if not all(col in df.columns for col in x_cols + y_cols):\n",
        "        return df\n",
        "\n",
        "    # 1. 원점(0,0) 혹은 중심으로부터의 거리 평균 (Shape Magnitude)\n",
        "    # 각 점들이 얼마나 퍼져있는가?\n",
        "    dists = []\n",
        "    for xc, yc in zip(x_cols, y_cols):\n",
        "        dists.append(np.sqrt(df[xc]**2 + df[yc]**2))\n",
        "\n",
        "    df['Feat_Geo_Mean_Dist'] = np.mean(dists, axis=0)\n",
        "    df['Feat_Geo_Max_Dist'] = np.max(dists, axis=0)\n",
        "    df['Feat_Geo_Std_Dist'] = np.std(dists, axis=0) # 형상의 불규칙성\n",
        "\n",
        "    # 2. 형상 비대칭성 (X축, Y축 범위 비율)\n",
        "    x_range = df[x_cols].max(axis=1) - df[x_cols].min(axis=1)\n",
        "    y_range = df[y_cols].max(axis=1) - df[y_cols].min(axis=1)\n",
        "    df['Feat_Geo_Aspect_Ratio'] = x_range / (y_range + 1e-6)\n",
        "\n",
        "    # 3. 다각형 면적 (Shoelace Formula: 신발끈 공식)\n",
        "    # 5개의 점이 이루는 내부 면적 계산\n",
        "    area = 0\n",
        "    for i in range(5):\n",
        "        j = (i + 1) % 5\n",
        "        # x_i * y_j - x_j * y_i\n",
        "        term = df[f'X{i+1}'] * df[f'Y{j+1}'] - df[f'X{j+1}'] * df[f'Y{i+1}']\n",
        "        area += term\n",
        "    df['Feat_Geo_Area'] = np.abs(area) / 2.0\n",
        "\n",
        "    return df\n",
        "\n",
        "def engineer_process_params(X):\n",
        "    \"\"\"\n",
        "    3. 공정 파라미터(Proc_Param) 통계 처리\n",
        "    - 파라미터 간의 편차(Instability) 확인\n",
        "    \"\"\"\n",
        "    df = X.copy()\n",
        "    proc_cols = [c for c in df.columns if 'Proc_Param' in c]\n",
        "\n",
        "    if proc_cols:\n",
        "        # 공정 파라미터들을 정규화했다고 가정하고, 행(Row)별 통계를 냄\n",
        "        # 의미: \"이 타이어 생산 시 공정 세팅이 얼마나 극단적이었나?\"\n",
        "        df['Feat_Proc_Mean'] = df[proc_cols].mean(axis=1)\n",
        "        df['Feat_Proc_Std'] = df[proc_cols].std(axis=1)  # 세팅의 변동성\n",
        "        df['Feat_Proc_Max'] = df[proc_cols].max(axis=1)  # 가장 강했던 공정 부하\n",
        "\n",
        "        # PCA 등을 쓰지 않고 간단한 상호작용 (선택적)\n",
        "        # 예: 파라미터1 * 파라미터2 (도메인 지식 필요하므로 여기선 제외)\n",
        "\n",
        "    return df\n",
        "\n",
        "def drop_raw_simulation(X):\n",
        "    \"\"\"\n",
        "    4. DNN 압축 완료된 원본 고차원 시뮬레이션 데이터 제거\n",
        "    \"\"\"\n",
        "    df = X.copy()\n",
        "    # x0~x255, y0~y255, p0~p255 패턴 제거\n",
        "    drop_cols = [c for c in df.columns if c.startswith(('x', 'y', 'p')) and c[1:].isdigit()]\n",
        "\n",
        "    # 대소문자 주의 (X1~X5는 대문자이므로 유지됨. x0~x255 소문자 제거)\n",
        "    # 혹시 모를 G1~G4(결과값)도 Feature로 쓰면 안되므로(Data Leakage) 제외해야 함 (Train에선 제외)\n",
        "    # 여기서는 x, y, p만 명시적으로 제거\n",
        "    if drop_cols:\n",
        "        df = df.drop(columns=drop_cols)\n",
        "\n",
        "    return df\n",
        "\n",
        "def encode_target(X):\n",
        "    \"\"\"\n",
        "    5. Class 라벨 인코딩 (NG=1, Good=0)\n",
        "    \"\"\"\n",
        "    df = X.copy()\n",
        "    if 'Class' in df.columns:\n",
        "        # NG가 불량(Positive case)인 경우가 많음\n",
        "        df['Class'] = df['Class'].apply(lambda x: 1 if 'NG' in str(x) else 0)\n",
        "    return df\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 실행 파이프라인\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# 예시용 더미 데이터 생성 (실제 사용 시엔 생략)\n",
        "# X = pd.read_csv('your_data.csv')\n",
        "\n",
        "print(\"--- 1. Spec Engineering 진행 중 (타이어 물리량 계산)...\")\n",
        "X = engineer_specs(X)\n",
        "\n",
        "print(\"--- 2. Geometry Engineering 진행 중 (형상 벡터 면적/거리 계산)...\")\n",
        "X = engineer_geometry(X)\n",
        "\n",
        "print(\"--- 3. Process Param Engineering 진행 중 (공정 변수 통계 추출)...\")\n",
        "X = engineer_process_params(X)\n",
        "\n",
        "print(\"--- 4. Raw Simulation Data Cleaning 진행 중 (DNN 압축 대체된 원본 제거)...\")\n",
        "X = drop_raw_simulation(X)\n",
        "\n",
        "print(\"--- 5. Target Encoding 진행 중 (NG/Good -> 1/0)...\")\n",
        "X = encode_target(X)\n",
        "\n",
        "print(\"\\n✅ 모든 Feature Engineering 완료!\")\n",
        "print(f\"최종 데이터 크기: {X.shape}\")\n",
        "print(\"생성된 파생 변수 예시:\")\n",
        "new_feats = [c for c in X.columns if c.startswith('Feat')]\n",
        "print(X[new_feats].head(2).T)"
      ],
      "metadata": {
        "id": "48ltMts09pFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if include_fem:\n",
        "  X_fem_ext = df_fem_feat\n",
        "  X = pd.concat([X, X_fem_ext], axis=1)\n",
        "\n",
        "X_train, X_test, cl_train, cl_test = train_test_split(X, cl, train_size=0.8)\n",
        "if augment:\n",
        "  X_train, cl_train = apply_smote(X_train, cl_train)"
      ],
      "metadata": {
        "id": "jZC0Xgob9xph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyZE7i89BS2G"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, cl_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-iFtLq-E2mL"
      },
      "outputs": [],
      "source": [
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syPU8bkfIIUT"
      },
      "outputs": [],
      "source": [
        "shap_value_pos = shap_values[:, :, 0]\n",
        "shap.summary_plot(shap_value_pos, X_test, plot_type=\"dot\", show=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XLmj-PtSBPw"
      },
      "outputs": [],
      "source": [
        "ps, ys = cross_validate(model, X, cl, cv=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GagjBNaxSwwu"
      },
      "outputs": [],
      "source": [
        "p = ps.flatten()\n",
        "y = ys.flatten()\n",
        "mask = (y == 0)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "upper = 0.1\n",
        "bins = np.linspace(0, upper, 11)\n",
        "freq_n, _, im1 = axes[0].hist(p[mask], bins=bins, alpha=0.7)\n",
        "freq_p, _, im2 = axes[0].hist(p[~mask], bins=bins, alpha=0.7)\n",
        "\n",
        "acc_n = np.cumsum(freq_n)\n",
        "acc_p = np.cumsum(freq_p)\n",
        "\n",
        "p_pred = (bins[1:] + bins[:-1]) / 2\n",
        "\n",
        "ratio = freq_p / (freq_n + freq_p)\n",
        "\n",
        "bin_indices = np.digitize(p, bins)\n",
        "bin_indices = np.clip(bin_indices, 1, len(bins) - 1) - 1\n",
        "p_bin = ratio[bin_indices]\n",
        "\n",
        "axes[1].plot(p_pred, ratio)\n",
        "axes[1].scatter(p, p_bin, s=3, alpha=0.5, c=y, **tab10_kwargs)\n",
        "axes[1].set_xlim(-upper * 0.1, upper * 1.1)\n",
        "axes[1].set_ylim(-upper * 0.1, upper * 1.1)\n",
        "axes[1].plot([0, upper], [0, upper], c='grey', lw=0.7, ls='--')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVOWPuPqz5FD"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "auroc = plot_curves(p, y, axes[0, 0], curve_type='auroc', n=2000)\n",
        "auprc = plot_curves(p, y, axes[0, 1], curve_type='auprc', n=2000)\n",
        "npv = plot_curves(p, y, axes[1, 0], curve_type='NPV', n=2000)\n",
        "p_actual = plot_curves(p, y, axes[1, 1], curve_type='profit', n=2000)\n",
        "thr_opt, p_mean, p_std, p_dec = plot_profit(p, y, axes[1, 1], quantile=[0.05, 0.25, 0.75, 0.95], n=1000, xlim=(0, 0.1), ylim=(-10000, 21000))\n",
        "print(f\"final score (predicted): {final_score(auroc, p_mean, p_std):.4f}\")\n",
        "print(f\"final score (actual upper bound): {final_score(auroc, p_actual, p_std):.4f}\")\n",
        "print(f\"final score (decision): {final_score(auroc, p_dec, p_std):.4f}\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_fem_exam\n",
        "pub_set_exam = FEMDataset(X_fem_exam, labels=X_sum_exam[target_cols])\n",
        "pub_loader_exam = DataLoader(pub_set_exam, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "rep_vs_exam = []\n",
        "model_fem.eval()\n",
        "for batch in pub_loader_exam:\n",
        "  x, y, p, _ = batch\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  p = p.to(device)\n",
        "  with torch.no_grad():\n",
        "    rep_vs_exam.append(model_fem.get_repv(x, y, p))\n",
        "rep_vs_exam = torch.cat(rep_vs_exam, dim=0).detach().cpu().numpy()\n",
        "\n",
        "df_fem_feat_exam = pd.DataFrame(\n",
        "    data = rep_vs_exam,\n",
        "    columns = [f'FEM_feat_{i}' for i in range(rep_vs.shape[1])]\n",
        ")\n",
        "df_fem_feat_exam.to_csv('data/fem_ext.csv')\n",
        "df_fem_feat_exam"
      ],
      "metadata": {
        "id": "1lyqys43jXf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39rEQV2xjkXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AQfL6Uh3iII"
      },
      "outputs": [],
      "source": [
        "model.fit(X, y)\n",
        "\n",
        "if model_type in ['rf', 'xgb']:\n",
        "  X_sum_exam = numerize(X_sum_exam, oe=oe)\n",
        "\n",
        "print(\"--- 1. Spec Engineering 진행 중 (타이어 물리량 계산)...\")\n",
        "X_sum_exam = engineer_specs(X_sum_exam)\n",
        "\n",
        "print(\"--- 2. Geometry Engineering 진행 중 (형상 벡터 면적/거리 계산)...\")\n",
        "X_sum_exam = engineer_geometry(X_sum_exam)\n",
        "\n",
        "print(\"--- 3. Process Param Engineering 진행 중 (공정 변수 통계 추출)...\")\n",
        "X_sum_exam = engineer_process_params(X_sum_exam)\n",
        "\n",
        "print(\"--- 4. Raw Simulation Data Cleaning 진행 중 (DNN 압축 대체된 원본 제거)...\")\n",
        "X_sum_exam = drop_raw_simulation(X_sum_exam)\n",
        "\n",
        "print(\"--- 5. Target Encoding 진행 중 (NG/Good -> 1/0)...\")\n",
        "X_sum_exam = encode_target(X_sum_exam)\n",
        "\n",
        "if include_fem:\n",
        "  X_fem_ext_exam = df_fem_feat_exam\n",
        "  X_exam = pd.concat([X_sum_exam, X_fem_ext_exam], axis=1)\n",
        "\n",
        "# 예측 수행 (X 대신 X_sum_exam 사용)\n",
        "p_exam = model.predict_proba(X_exam)[:, 1]\n",
        "\n",
        "ax = plt.subplot()\n",
        "thr, p_mean, p_std, _ = plot_profit(p_hat=p_exam, y_true=None, ax=ax, n=1000, xlim=(0, 0.1), ylim=(-10000, 10000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL5TdBBtArfh"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "thr = 0.03\n",
        "decision = (p_exam < thr)\n",
        "print(f\"# of selected products: {decision.sum()}\")\n",
        "\n",
        "submission = pd.read_csv('data/sample_submission.csv')\n",
        "\n",
        "submission['probability'] = np.concatenate([p_exam, p_exam])\n",
        "submission['decision'] = np.concatenate([decision, decision])\n",
        "\n",
        "now = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=9))).strftime(\"%m-%d-%H-%M\")\n",
        "submission.to_csv(f\"submission_HAIYONG_{now}.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}