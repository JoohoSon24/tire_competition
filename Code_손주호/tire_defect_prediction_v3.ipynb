{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tire Defect Prediction V3 - Production-Ready Profit-Optimized Model\n",
    "## Advanced Manufacturing AI with Organized File Structure\n",
    "\n",
    "**Version 3 Improvements:**\n",
    "- Organized file structure: `model_param/` for models, `results/` for submissions\n",
    "- Profit-aware training with custom loss function\n",
    "- Advanced DNN architecture with Attention and Residual connections\n",
    "- Multi-model ensemble (XGBoost + CatBoost + LightGBM)\n",
    "- Automated threshold optimization integrated in training\n",
    "- Enhanced feature engineering\n",
    "- Automatic submission generation with optimal threshold\n",
    "\n",
    "**File Structure:**\n",
    "```\n",
    "Code_손주호/\n",
    "├── model_param/          # All .pth and .pkl model files\n",
    "├── results/             # All submission CSV files\n",
    "├── data/                # Training and test data\n",
    "└── past_versions/       # Previous notebook versions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] LightGBM not available. Install with: pip install lightgbm\n",
      "[INFO] Created directories: model_param/, results/\n",
      "Using device: cpu\n",
      "\n",
      "============================================================\n",
      "PROFIT MODEL CONFIGURATION\n",
      "============================================================\n",
      "Cost of False Positive:  $1.0\n",
      "Cost of False Negative:  $10.0 (Critical!)\n",
      "Benefit of True Positive: $0.5\n",
      "Benefit of True Negative: $0.1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "import warnings\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Boosting Libraries\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LIGHTGBM = True\n",
    "except ImportError:\n",
    "    HAS_LIGHTGBM = False\n",
    "    print(\"[WARNING] LightGBM not available. Install with: pip install lightgbm\")\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('model_param', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "print(\"[INFO] Created directories: model_param/, results/\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Business Parameters - ADJUST THESE BASED ON YOUR USE CASE\n",
    "COST_FP = 1.0    # Cost of rejecting good tire\n",
    "COST_FN = 10.0   # Cost of accepting defective tire (typically much higher!)\n",
    "BENEFIT_TP = 0.5 # Benefit of catching defective tire\n",
    "BENEFIT_TN = 0.1 # Benefit of accepting good tire\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFIT MODEL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Cost of False Positive:  ${COST_FP}\")\n",
    "print(f\"Cost of False Negative:  ${COST_FN} (Critical!)\")\n",
    "print(f\"Benefit of True Positive: ${BENEFIT_TP}\")\n",
    "print(f\"Benefit of True Negative: ${BENEFIT_TN}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n",
      "\n",
      "Train shape: (720, 799)\n",
      "Test shape:  (466, 799)\n",
      "\n",
      "Target Distribution:\n",
      "Class\n",
      "0    613\n",
      "1    107\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Defect Rate: 14.86%\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"[INFO] Loading data...\")\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"\\nTrain shape: {df_train.shape}\")\n",
    "print(f\"Test shape:  {df_test.shape}\")\n",
    "\n",
    "# Convert target\n",
    "df_train['Class'] = (df_train['Class'] == 'NG').astype(int)\n",
    "\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(df_train['Class'].value_counts())\n",
    "print(f\"\\nDefect Rate: {df_train['Class'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design Features: 16\n",
      "Simulation Features: 772\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups\n",
    "DESIGN_FEATURES = ['Mass_Pilot', 'Width', 'Aspect', 'Inch', 'Plant'] + \\\n",
    "                  [f'Proc_Param{i}' for i in range(1, 12)]\n",
    "\n",
    "SIMULATION_FEATURES = [f'x{i}' for i in range(0, 256)] + \\\n",
    "                      [f'y{i}' for i in range(0, 256)] + \\\n",
    "                      [f'p{i}' for i in range(0, 256)] + \\\n",
    "                      [f'G{i}' for i in range(1, 5)]\n",
    "SIMULATION_FEATURES = list(set(SIMULATION_FEATURES))\n",
    "SIMULATION_FEATURES.sort()\n",
    "\n",
    "TARGET = 'Label Class'\n",
    "\n",
    "print(f\"Design Features: {len(DESIGN_FEATURES)}\")\n",
    "print(f\"Simulation Features: {len(SIMULATION_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Label Class'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     69\u001b[39m preprocessor = AdvancedFeaturePreprocessor(use_robust_scaler=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     70\u001b[39m df_processed = preprocessor.fit_transform(df_train, DESIGN_FEATURES, SIMULATION_FEATURES, TARGET)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m X = \u001b[43mdf_processed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTARGET\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m y = df_processed[TARGET]\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessed shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Aanaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Aanaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Aanaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Aanaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7098\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7097\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7098\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7099\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Label Class'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Enhanced Preprocessing\n",
    "class AdvancedFeaturePreprocessor:\n",
    "    def __init__(self, use_robust_scaler=True):\n",
    "        self.use_robust_scaler = use_robust_scaler\n",
    "        self.label_encoders = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "    def fit_transform(self, df, design_features, simulation_features, target_col):\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in df_processed.columns:\n",
    "            if df_processed[col].isnull().sum() > 0:\n",
    "                if df_processed[col].dtype in ['float64', 'int64']:\n",
    "                    df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "                else:\n",
    "                    df_processed[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        # Encode categorical\n",
    "        categorical_features = ['Mass_Pilot', 'Plant','Proc_Param6']\n",
    "        for feature in categorical_features:\n",
    "            if feature in df_processed.columns:\n",
    "                le = LabelEncoder()\n",
    "                df_processed[feature] = le.fit_transform(df_processed[feature].astype(str))\n",
    "                self.label_encoders[feature] = le\n",
    "        \n",
    "        # Scale features\n",
    "        ScalerClass = RobustScaler if self.use_robust_scaler else StandardScaler\n",
    "        \n",
    "        for group_name, features in [('design', design_features), ('simulation', simulation_features)]:\n",
    "            existing_features = [f for f in features if f in df_processed.columns]\n",
    "            if len(existing_features) > 0:\n",
    "                scaler = ScalerClass()\n",
    "                df_processed[existing_features] = scaler.fit_transform(df_processed[existing_features])\n",
    "                self.scalers[group_name] = scaler\n",
    "        \n",
    "        self.design_features = design_features\n",
    "        self.simulation_features = simulation_features\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def transform(self, df):\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        for col in df_processed.columns:\n",
    "            if df_processed[col].isnull().sum() > 0:\n",
    "                if df_processed[col].dtype in ['float64', 'int64']:\n",
    "                    df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "                else:\n",
    "                    df_processed[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        # Encode categorical\n",
    "        for feature, le in self.label_encoders.items():\n",
    "            if feature in df_processed.columns:\n",
    "                df_processed[feature] = df_processed[feature].astype(str).map(\n",
    "                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                )\n",
    "        \n",
    "        # Scale features\n",
    "        for group_name, features in [('design', self.design_features), ('simulation', self.simulation_features)]:\n",
    "            existing_features = [f for f in features if f in df_processed.columns]\n",
    "            if len(existing_features) > 0 and group_name in self.scalers:\n",
    "                df_processed[existing_features] = self.scalers[group_name].transform(df_processed[existing_features])\n",
    "        \n",
    "        return df_processed\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessor = AdvancedFeaturePreprocessor(use_robust_scaler=True)\n",
    "df_processed = preprocessor.fit_transform(df_train, DESIGN_FEATURES, SIMULATION_FEATURES, TARGET)\n",
    "\n",
    "X = df_processed.drop(columns=[TARGET])\n",
    "y = df_processed[TARGET]\n",
    "\n",
    "print(f\"\\nProcessed shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick feature selection\n",
    "print(\"[INFO] Performing feature selection...\")\n",
    "X_train_fs, X_val_fs, y_train_fs, y_val_fs = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "xgb_fs = xgb.XGBClassifier(n_estimators=100, max_depth=5, random_state=SEED, tree_method='hist')\n",
    "xgb_fs.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_fs.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "TOP_N = 100\n",
    "selected_features = feature_importance.head(TOP_N)['feature'].tolist()\n",
    "selected_design_features = [f for f in selected_features if f in DESIGN_FEATURES]\n",
    "selected_simulation_features = [f for f in selected_features if f in SIMULATION_FEATURES]\n",
    "\n",
    "X_design = X[selected_design_features]\n",
    "X_simulation = X[selected_simulation_features]\n",
    "\n",
    "print(f\"Selected: {len(selected_design_features)} design + {len(selected_simulation_features)} simulation features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Self-attention layer for feature importance weighting\"\"\"\n",
    "    def __init__(self, input_dim, attention_dim=32):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_weights = F.softmax(self.attention(x), dim=1)\n",
    "        weighted = x * attention_weights\n",
    "        return weighted\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with skip connections\"\"\"\n",
    "    def __init__(self, dim, dropout_rate=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.bn1 = nn.BatchNorm1d(dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.bn2 = nn.BatchNorm1d(dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.fc1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.fc2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AdvancedSimulationDNN(nn.Module):\n",
    "    \"\"\"Enhanced DNN with Attention and Residual connections\"\"\"\n",
    "    def __init__(self, input_dim, latent_dim=128, dropout_rate=0.3):\n",
    "        super(AdvancedSimulationDNN, self).__init__()\n",
    "        \n",
    "        # Attention on input\n",
    "        self.attention = AttentionLayer(input_dim, attention_dim=64)\n",
    "        \n",
    "        # Encoder with residual blocks\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_block1 = ResidualBlock(256, dropout_rate)\n",
    "        self.res_block2 = ResidualBlock(256, dropout_rate)\n",
    "        \n",
    "        # Latent representation\n",
    "        self.to_latent = nn.Sequential(\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Predictor\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention\n",
    "        x_attended = self.attention(x)\n",
    "        \n",
    "        # Encode\n",
    "        encoded = self.encoder(x_attended)\n",
    "        \n",
    "        # Residual blocks\n",
    "        encoded = self.res_block1(encoded)\n",
    "        encoded = self.res_block2(encoded)\n",
    "        \n",
    "        # Latent\n",
    "        latent = self.to_latent(encoded)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.predictor(latent)\n",
    "        \n",
    "        return latent, prediction\n",
    "\n",
    "\n",
    "class AdvancedFusionModel(nn.Module):\n",
    "    \"\"\"Enhanced fusion model with multiple ensemble inputs\"\"\"\n",
    "    def __init__(self, n_boosting_models, boosting_latent_dim, dnn_latent_dim, dropout_rate=0.3):\n",
    "        super(AdvancedFusionModel, self).__init__()\n",
    "        \n",
    "        # Total input: predictions from each boosting model + latents\n",
    "        total_dim = n_boosting_models + boosting_latent_dim + dnn_latent_dim + 1\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(total_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, *inputs):\n",
    "        fused = torch.cat(inputs, dim=1)\n",
    "        output = self.fusion(fused)\n",
    "        return output\n",
    "\n",
    "print(\"[INFO] Advanced architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Model Ensemble Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_design_train, X_design_val, X_sim_train, X_sim_val, y_train, y_val = train_test_split(\n",
    "    X_design, X_simulation, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MULTI-MODEL ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train multiple boosting models\n",
    "boosting_models = {}\n",
    "boosting_predictions = {'train': {}, 'val': {}}\n",
    "\n",
    "# XGBoost\n",
    "print(\"\\n[1/3] Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=SEED,\n",
    "    tree_method='hist'\n",
    ")\n",
    "xgb_model.fit(X_design_train, y_train, eval_set=[(X_design_val, y_val)], verbose=False)\n",
    "boosting_models['xgb'] = xgb_model\n",
    "boosting_predictions['train']['xgb'] = xgb_model.predict_proba(X_design_train)[:, 1].reshape(-1, 1)\n",
    "boosting_predictions['val']['xgb'] = xgb_model.predict_proba(X_design_val)[:, 1].reshape(-1, 1)\n",
    "print(f\"   XGBoost trained. Val AUC: {roc_auc_score(y_val, boosting_predictions['val']['xgb']):.4f}\")\n",
    "\n",
    "# CatBoost\n",
    "print(\"\\n[2/3] Training CatBoost...\")\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=400,\n",
    "    depth=8,\n",
    "    learning_rate=0.03,\n",
    "    random_state=SEED,\n",
    "    verbose=False\n",
    ")\n",
    "cat_model.fit(X_design_train, y_train, eval_set=(X_design_val, y_val), verbose=False)\n",
    "boosting_models['cat'] = cat_model\n",
    "boosting_predictions['train']['cat'] = cat_model.predict_proba(X_design_train)[:, 1].reshape(-1, 1)\n",
    "boosting_predictions['val']['cat'] = cat_model.predict_proba(X_design_val)[:, 1].reshape(-1, 1)\n",
    "print(f\"   CatBoost trained. Val AUC: {roc_auc_score(y_val, boosting_predictions['val']['cat']):.4f}\")\n",
    "\n",
    "# LightGBM (if available)\n",
    "if HAS_LIGHTGBM:\n",
    "    print(\"\\n[3/3] Training LightGBM...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=SEED,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_design_train, y_train, eval_set=[(X_design_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    boosting_models['lgb'] = lgb_model\n",
    "    boosting_predictions['train']['lgb'] = lgb_model.predict_proba(X_design_train)[:, 1].reshape(-1, 1)\n",
    "    boosting_predictions['val']['lgb'] = lgb_model.predict_proba(X_design_val)[:, 1].reshape(-1, 1)\n",
    "    print(f\"   LightGBM trained. Val AUC: {roc_auc_score(y_val, boosting_predictions['val']['lgb']):.4f}\")\n",
    "else:\n",
    "    print(\"\\n[3/3] LightGBM skipped (not installed)\")\n",
    "\n",
    "# Get latent features from XGBoost (representative)\n",
    "xgb_latent_train = xgb_model.apply(X_design_train)\n",
    "xgb_latent_val = xgb_model.apply(X_design_val)\n",
    "scaler_xgb_latent = StandardScaler()\n",
    "xgb_latent_train_scaled = scaler_xgb_latent.fit_transform(xgb_latent_train)\n",
    "xgb_latent_val_scaled = scaler_xgb_latent.transform(xgb_latent_val)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BOOSTING ENSEMBLE COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Advanced DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ADVANCED DNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare tensors\n",
    "X_sim_train_tensor = torch.FloatTensor(X_sim_train.values).to(device)\n",
    "X_sim_val_tensor = torch.FloatTensor(X_sim_val.values).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1).to(device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_sim_train_tensor, y_train_tensor),\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "dnn_model = AdvancedSimulationDNN(\n",
    "    input_dim=X_sim_train.shape[1],\n",
    "    latent_dim=128,\n",
    "    dropout_rate=0.4\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(dnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# Training\n",
    "best_val_loss = float('inf')\n",
    "patience, patience_counter = 20, 0\n",
    "\n",
    "for epoch in range(150):\n",
    "    dnn_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        latent, pred = dnn_model(batch_X)\n",
    "        loss = criterion(pred, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(dnn_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    dnn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_latent, val_pred = dnn_model(X_sim_val_tensor)\n",
    "        val_loss = criterion(val_pred, y_val_tensor).item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/150] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(dnn_model.state_dict(), 'model_param/best_dnn_v3.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best\n",
    "dnn_model.load_state_dict(torch.load('model_param/best_dnn_v3.pth'))\n",
    "dnn_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dnn_latent_train, dnn_pred_train = dnn_model(X_sim_train_tensor)\n",
    "    dnn_latent_val, dnn_pred_val = dnn_model(X_sim_val_tensor)\n",
    "\n",
    "dnn_val_pred_prob = torch.sigmoid(dnn_pred_val).cpu().numpy()\n",
    "print(f\"\\nDNN Val AUC: {roc_auc_score(y_val, dnn_val_pred_prob):.4f}\")\n",
    "print(f\"[SAVED] model_param/best_dnn_v3.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DNN TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING FUSION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare fusion inputs\n",
    "boosting_preds_train = [torch.FloatTensor(boosting_predictions['train'][m]).to(device) \n",
    "                        for m in boosting_predictions['train']]\n",
    "boosting_preds_val = [torch.FloatTensor(boosting_predictions['val'][m]).to(device) \n",
    "                      for m in boosting_predictions['val']]\n",
    "\n",
    "xgb_latent_train_tensor = torch.FloatTensor(xgb_latent_train_scaled).to(device)\n",
    "xgb_latent_val_tensor = torch.FloatTensor(xgb_latent_val_scaled).to(device)\n",
    "\n",
    "# Create datasets\n",
    "fusion_train_data = TensorDataset(\n",
    "    *boosting_preds_train,\n",
    "    xgb_latent_train_tensor,\n",
    "    dnn_latent_train,\n",
    "    dnn_pred_train,\n",
    "    y_train_tensor\n",
    ")\n",
    "\n",
    "fusion_val_data = TensorDataset(\n",
    "    *boosting_preds_val,\n",
    "    xgb_latent_val_tensor,\n",
    "    dnn_latent_val,\n",
    "    dnn_pred_val,\n",
    "    y_val_tensor\n",
    ")\n",
    "\n",
    "fusion_train_loader = DataLoader(fusion_train_data, batch_size=128, shuffle=True)\n",
    "fusion_val_loader = DataLoader(fusion_val_data, batch_size=256, shuffle=False)\n",
    "\n",
    "# Initialize fusion model\n",
    "n_boosting = len(boosting_models)\n",
    "fusion_model = AdvancedFusionModel(\n",
    "    n_boosting_models=n_boosting,\n",
    "    boosting_latent_dim=xgb_latent_train.shape[1],\n",
    "    dnn_latent_dim=128,\n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "criterion_fusion = nn.BCEWithLogitsLoss()\n",
    "optimizer_fusion = optim.AdamW(fusion_model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler_fusion = optim.lr_scheduler.ReduceLROnPlateau(optimizer_fusion, mode='min', patience=7)\n",
    "\n",
    "# Training\n",
    "best_val_loss_fusion = float('inf')\n",
    "patience_fusion, patience_counter_fusion = 15, 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    fusion_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_data in fusion_train_loader:\n",
    "        *inputs, labels = batch_data\n",
    "        optimizer_fusion.zero_grad()\n",
    "        output = fusion_model(*inputs)\n",
    "        loss = criterion_fusion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer_fusion.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(fusion_train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    fusion_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in fusion_val_loader:\n",
    "            *inputs, labels = batch_data\n",
    "            output = fusion_model(*inputs)\n",
    "            val_loss += criterion_fusion(output, labels).item()\n",
    "    \n",
    "    val_loss /= len(fusion_val_loader)\n",
    "    scheduler_fusion.step(val_loss)\n",
    "    \n",
    "    if (epoch + 1) % 15 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/100] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss_fusion:\n",
    "        best_val_loss_fusion = val_loss\n",
    "        patience_counter_fusion = 0\n",
    "        torch.save(fusion_model.state_dict(), 'model_param/best_fusion_v3.pth')\n",
    "    else:\n",
    "        patience_counter_fusion += 1\n",
    "        if patience_counter_fusion >= patience_fusion:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best\n",
    "fusion_model.load_state_dict(torch.load('model_param/best_fusion_v3.pth'))\n",
    "print(f\"[SAVED] model_param/best_fusion_v3.pth\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FUSION TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Profit Curve Analysis and Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final predictions\n",
    "fusion_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_logits_val = fusion_model(*boosting_preds_val, xgb_latent_val_tensor, dnn_latent_val, dnn_pred_val)\n",
    "    final_pred_probs_val = torch.sigmoid(final_logits_val).cpu().numpy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFIT CURVE ANALYSIS & THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_profit(y_true, y_pred_probs, threshold):\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int).flatten()\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    profit = (TP * BENEFIT_TP) + (TN * BENEFIT_TN) - (FP * COST_FP) - (FN * COST_FN)\n",
    "    return profit, TP, TN, FP, FN\n",
    "\n",
    "# Test thresholds\n",
    "thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "profits = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    profit, _, _, _, _ = calculate_profit(y_val.values, final_pred_probs_val, threshold)\n",
    "    profits.append(profit)\n",
    "\n",
    "# Find optimal\n",
    "optimal_idx = np.argmax(profits)\n",
    "OPTIMAL_THRESHOLD = thresholds[optimal_idx]\n",
    "optimal_profit = profits[optimal_idx]\n",
    "\n",
    "print(f\"\\n[RESULT] Optimal Threshold: {OPTIMAL_THRESHOLD:.3f}\")\n",
    "print(f\"[RESULT] Maximum Profit: ${optimal_profit:.2f}\")\n",
    "\n",
    "_, opt_tp, opt_tn, opt_fp, opt_fn = calculate_profit(y_val.values, final_pred_probs_val, OPTIMAL_THRESHOLD)\n",
    "print(f\"\\n[CONFUSION MATRIX at Optimal Threshold]\")\n",
    "print(f\"   TP: {opt_tp} | TN: {opt_tn}\")\n",
    "print(f\"   FP: {opt_fp} | FN: {opt_fn}\")\n",
    "\n",
    "y_pred_optimal = (final_pred_probs_val >= OPTIMAL_THRESHOLD).astype(int)\n",
    "print(f\"\\n[METRICS at Optimal Threshold]\")\n",
    "print(f\"   Accuracy:  {accuracy_score(y_val, y_pred_optimal):.4f}\")\n",
    "print(f\"   Precision: {precision_score(y_val, y_pred_optimal):.4f}\")\n",
    "print(f\"   Recall:    {recall_score(y_val, y_pred_optimal):.4f}\")\n",
    "print(f\"   F1-Score:  {f1_score(y_val, y_pred_optimal):.4f}\")\n",
    "print(f\"   ROC-AUC:   {roc_auc_score(y_val, final_pred_probs_val):.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(thresholds, profits, linewidth=2, color='green')\n",
    "plt.axvline(OPTIMAL_THRESHOLD, color='red', linestyle='--', label=f'Optimal: {OPTIMAL_THRESHOLD:.3f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Profit ($)')\n",
    "plt.title('Profit Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cm = confusion_matrix(y_val, y_pred_optimal)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix (Threshold={OPTIMAL_THRESHOLD:.3f})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROFIT ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Submission with Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Preprocess test data\n",
    "df_test_processed = preprocessor.transform(df_test)\n",
    "X_test_design = df_test_processed[selected_design_features]\n",
    "X_test_simulation = df_test_processed[selected_simulation_features]\n",
    "\n",
    "# Get boosting predictions\n",
    "test_boosting_preds = []\n",
    "for model_name, model in boosting_models.items():\n",
    "    pred = model.predict_proba(X_test_design)[:, 1].reshape(-1, 1)\n",
    "    test_boosting_preds.append(torch.FloatTensor(pred).to(device))\n",
    "\n",
    "# Get XGBoost latent\n",
    "xgb_latent_test = xgb_model.apply(X_test_design)\n",
    "xgb_latent_test_scaled = scaler_xgb_latent.transform(xgb_latent_test)\n",
    "xgb_latent_test_tensor = torch.FloatTensor(xgb_latent_test_scaled).to(device)\n",
    "\n",
    "# Get DNN predictions\n",
    "X_test_sim_tensor = torch.FloatTensor(X_test_simulation.values).to(device)\n",
    "dnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    dnn_latent_test, dnn_pred_test = dnn_model(X_test_sim_tensor)\n",
    "\n",
    "# Get fusion predictions\n",
    "fusion_model.eval()\n",
    "with torch.no_grad():\n",
    "    final_logits_test = fusion_model(*test_boosting_preds, xgb_latent_test_tensor, dnn_latent_test, dnn_pred_test)\n",
    "    final_pred_probs_test = torch.sigmoid(final_logits_test).cpu().numpy()\n",
    "\n",
    "# Create submission\n",
    "test_ids = df_test['ID'] if 'ID' in df_test.columns else [f\"ID_{i}_L\" for i in range(len(df_test))]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'probability': final_pred_probs_test.flatten(),\n",
    "    'decision': (final_pred_probs_test.flatten() >= OPTIMAL_THRESHOLD)\n",
    "})\n",
    "\n",
    "# Save\n",
    "now = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=9))).strftime(\"%m-%d-%H-%M\")\n",
    "output_file = f\"results/submission_V3_PROFIT_OPTIMIZED_{now}.csv\"\n",
    "submission.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n[INFO] Predictions generated:\")\n",
    "print(f\"   - Total samples: {len(submission)}\")\n",
    "print(f\"   - Probability range: [{final_pred_probs_test.min():.4f}, {final_pred_probs_test.max():.4f}]\")\n",
    "print(f\"   - Decision threshold: {OPTIMAL_THRESHOLD:.3f}\")\n",
    "print(f\"   - Predicted as defective (NG): {submission['decision'].sum()} ({submission['decision'].mean():.1%})\")\n",
    "print(f\"\\n[SAVED] {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUBMISSION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save All Models and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING ALL MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save all artifacts to model_param folder\n",
    "with open('model_param/preprocessor_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "print(\"[SAVED] model_param/preprocessor_v3.pkl\")\n",
    "\n",
    "with open('model_param/boosting_models_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(boosting_models, f)\n",
    "print(\"[SAVED] model_param/boosting_models_v3.pkl\")\n",
    "\n",
    "with open('model_param/scaler_xgb_latent_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_xgb_latent, f)\n",
    "print(\"[SAVED] model_param/scaler_xgb_latent_v3.pkl\")\n",
    "\n",
    "config = {\n",
    "    'selected_design_features': selected_design_features,\n",
    "    'selected_simulation_features': selected_simulation_features,\n",
    "    'optimal_threshold': OPTIMAL_THRESHOLD,\n",
    "    'cost_fp': COST_FP,\n",
    "    'cost_fn': COST_FN,\n",
    "    'benefit_tp': BENEFIT_TP,\n",
    "    'benefit_tn': BENEFIT_TN,\n",
    "    'model_version': 'v3',\n",
    "    'training_date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('model_param/config_v3.pkl', 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "print(\"[SAVED] model_param/config_v3.pkl\")\n",
    "\n",
    "print(\"\\n[INFO] All model files saved to: model_param/\")\n",
    "print(\"[INFO] Submission file saved to: results/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROJECT V3 COMPLETE - PRODUCTION READY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print file structure summary\n",
    "print(\"\\n[FILE STRUCTURE]\")\n",
    "print(\"model_param/\")\n",
    "print(\"  ├── best_dnn_v3.pth\")\n",
    "print(\"  ├── best_fusion_v3.pth\")\n",
    "print(\"  ├── preprocessor_v3.pkl\")\n",
    "print(\"  ├── boosting_models_v3.pkl\")\n",
    "print(\"  ├── scaler_xgb_latent_v3.pkl\")\n",
    "print(\"  └── config_v3.pkl\")\n",
    "print(\"\\nresults/\")\n",
    "print(f\"  └── submission_V3_PROFIT_OPTIMIZED_{now}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
