# Main Model Documentation - Tire Defect Prediction

## ğŸ“Š í”„ë¡œì íŠ¸ ê°œìš”

**íŒŒì¼ëª…**: `main copy.ipynb`
**ëª©ì **: íƒ€ì´ì–´ ë¶ˆëŸ‰ ì˜ˆì¸¡ ë° ìµœì  ì„ ë³„ ì˜ì‚¬ê²°ì •
**ë¬¸ì œ ìœ í˜•**: Binary Classification with Business Optimization
**í•µì‹¬ ëª©í‘œ**: Profit ìµœëŒ€í™”ë¥¼ ìœ„í•œ ë¶ˆëŸ‰ í™•ë¥  ì˜ˆì¸¡ ë° ì„ê³„ê°’ ìµœì í™”

---

## ğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ: Profit ì •ì˜

### âš ï¸ **PROFIT ê³„ì‚°ì‹ (í•µì‹¬ ê°œë…)**

ì´ í”„ë¡œì íŠ¸ì˜ ê°€ì¥ ì¤‘ìš”í•œ í‰ê°€ ì§€í‘œëŠ” **Profit(ì´ìµ)**ì…ë‹ˆë‹¤.

```python
# Profit ê³„ì‚° ê³µì‹
Profit = (TN Ã— 100) - (FN Ã— 2000) - Penalty

where:
    TN  = True Negative  (Goodì„ Goodìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡)
    FN  = False Negative (NGë¥¼ Goodìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡)
    Penalty = 99,999 (if ì„ íƒëœ ì œí’ˆ ìˆ˜ > 200ê°œ ê¸°ì¤€)
```

### ğŸ’° **ì„¸ë¶€ ë¹„ìš© êµ¬ì¡°**

| ì˜ˆì¸¡ ê²°ê³¼ | ì‹¤ì œ ìƒíƒœ | ì˜ì‚¬ê²°ì • | ë¹„ìš©/ìˆ˜ìµ | ì„¤ëª… |
|-----------|----------|----------|-----------|------|
| **Good ì˜ˆì¸¡ (ì„ íƒ)** | Good | ì¶œí•˜ | **+100ì›** | ì •ìƒ ì œí’ˆ íŒë§¤ ìˆ˜ìµ |
| **Good ì˜ˆì¸¡ (ì„ íƒ)** | NG | ì¶œí•˜ | **-2,000ì›** | ë¶ˆëŸ‰í’ˆ ì¶œí•˜ â†’ ê³ ê° í´ë ˆì„ ì†ì‹¤ |
| **NG ì˜ˆì¸¡ (ê±°ë¶€)** | NG | íê¸° | 0ì› | ë¶ˆëŸ‰í’ˆ ì‚¬ì „ ì°¨ë‹¨ (ì†ì‹¤ ì—†ìŒ) |
| **NG ì˜ˆì¸¡ (ê±°ë¶€)** | Good | íê¸° | 0ì› | ì •ìƒí’ˆ íê¸° (ê¸°íšŒë¹„ìš©, ê³„ì‚° ë¯¸í¬í•¨) |

### ğŸ“Œ **ì œì•½ ì¡°ê±´ (Penalty)**

```
IF ì„ íƒëœ ì œí’ˆ ìˆ˜ > (200 Ã— ì „ì²´ ë°ì´í„° ìˆ˜ / 466):
    Profit -= 99,999
```

- **ì˜ë¯¸**: ë„ˆë¬´ ë§ì€ ì œí’ˆì„ ì„ íƒí•˜ë©´ ëŒ€ëŸ‰ íŒ¨ë„í‹° ë¶€ê³¼
- **ë¹„ìœ¨**: ì „ì²´ ë°ì´í„°ì˜ ì•½ 42.9% (200/466) ì´ˆê³¼ ì‹œ
- **ì „ëµ**: ë³´ìˆ˜ì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ê³ í’ˆì§ˆ ì œí’ˆë§Œ ì¶œí•˜

### ğŸ² **Expected Profit (ê¸°ëŒ“ê°’ ê¸°ë°˜ ìµœì í™”)**

ì‹¤ì œ ê²€ì¦ ë°ì´í„°ê°€ ì—†ì„ ë•Œ, ì˜ˆì¸¡ í™•ë¥ ë¡œ ê¸°ëŒ“ê°’ì„ ê³„ì‚°:

```python
# ê° ìƒ˜í”Œì˜ ê¸°ëŒ“ê°’
Expected_Profit_per_sample = (1 - p) Ã— 100 - p Ã— 2000

where:
    p = ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë¶ˆëŸ‰ í™•ë¥  (0~1)

# ì „ì²´ ê¸°ëŒ“ê°’
Total_Expected_Profit = Î£(Expected_Profit_per_sample) - Penalty
```

**í•´ì„**:
- ë¶ˆëŸ‰ í™•ë¥  `p = 0.01` (1%) â†’ ê¸°ëŒ“ê°’ = 99 - 20 = **+79ì›**
- ë¶ˆëŸ‰ í™•ë¥  `p = 0.05` (5%) â†’ ê¸°ëŒ“ê°’ = 95 - 100 = **-5ì›** (ì†ì‹¤!)
- ë¶ˆëŸ‰ í™•ë¥  `p = 0.10` (10%) â†’ ê¸°ëŒ“ê°’ = 90 - 200 = **-110ì›** (í° ì†ì‹¤)

**ì„ê³„ê°’ ì˜ˆì‹œ**:
- `p < 0.048` â†’ ê¸°ëŒ“ê°’ > 0 (ì„ íƒ ê¶Œì¥)
- `p â‰¥ 0.048` â†’ ê¸°ëŒ“ê°’ â‰¤ 0 (ê±°ë¶€ ê¶Œì¥)

---

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### 1ï¸âƒ£ **ë°ì´í„° êµ¬ì¡°**

#### ì…ë ¥ ë°ì´í„°

**Feature Groups**:
1. **Summary Features (X_sum)**: ì„¤ê³„ ë° ê³µì • íŒŒë¼ë¯¸í„°
   - `Mass_Pilot`: ì–‘ì‚°/íŒŒì¼ëŸ¿ êµ¬ë¶„ (Boolean)
   - `Width`, `Aspect`, `Inch`: íƒ€ì´ì–´ ê·œê²©
   - `Plant`: ìƒì‚° ê³µì¥
   - `Proc_Param1` ~ `Proc_Param11`: ê³µì • íŒŒë¼ë¯¸í„°
   - `G1` ~ `G4`: í†µê³„ íŠ¹ì§•

2. **FEM Features (X_fem)**: ì‹œë®¬ë ˆì´ì…˜ ê³¡ì„  ë°ì´í„°
   - `x0` ~ `x255`: Xì¢Œí‘œ (256ê°œ í¬ì¸íŠ¸)
   - `y0` ~ `y255`: Yì¢Œí‘œ (256ê°œ í¬ì¸íŠ¸)
   - `p0` ~ `p255`: ì••ë ¥ ê°’ (256ê°œ í¬ì¸íŠ¸)

**Target Variable**:
- `Class`: 'Good' or 'NG' (Binary)

#### ë°ì´í„° ë¶„í•  í•¨ìˆ˜

```python
def split_data(df, train=True):
    """
    ë°ì´í„°ë¥¼ Summary íŠ¹ì§•ê³¼ FEM íŠ¹ì§•ìœ¼ë¡œ ë¶„ë¦¬

    Returns:
        - X_sum: ìš”ì•½ íŠ¹ì§• (ì„¤ê³„/ê³µì • íŒŒë¼ë¯¸í„°)
        - X_fem: FEM ì‹œë®¬ë ˆì´ì…˜ íŠ¹ì§• (x, y, p ê³¡ì„ )
        - y (train=True) or ids (train=False)
    """
```

---

### 2ï¸âƒ£ **Feature Engineering**

#### ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬

```python
def numerize(X_sum, oe=None):
    """
    ë²”ì£¼í˜• íŠ¹ì§•ì„ One-Hot Encodingìœ¼ë¡œ ë³€í™˜

    Categorical Features:
        - Mass_Pilot (Boolean)
        - Plant (ê³µì¥ ì½”ë“œ)
        - Proc_Param6 (ë²”ì£¼í˜• ê³µì • íŒŒë¼ë¯¸í„°)

    Returns:
        - X: One-Hot Encoded + Numerical Features
        - oe: OneHotEncoder ê°ì²´ (ì¬ì‚¬ìš©ìš©)
    """
```

#### FEM íŠ¹ì§• ì¶”ì¶œ (ì„ íƒ ì‚¬í•­)

ê³ ì°¨ì› FEM ë°ì´í„°(768ì°¨ì›)ë¥¼ ì €ì°¨ì› í†µê³„ íŠ¹ì§•ìœ¼ë¡œ ì••ì¶•:

```python
fem_feat_fns = [
    lambda x, y, p: p.max(axis=-1),    # ìµœëŒ€ ì••ë ¥
    curve_length,                       # ê³¡ì„  ì „ì²´ ê¸¸ì´
    stress_length,                      # ê³ ì‘ë ¥ êµ¬ê°„ ê¸¸ì´ (p > 2.5)
    bend_extent,                        # êµ½í˜ ì •ë„ (y_range / x_range)
    max_curvature                       # ìµœëŒ€ ê³¡ë¥ 
]
```

**í•¨ìˆ˜ ì„¸ë¶€ ì„¤ëª…**:

1. **`curve_length(x, y, p)`**: ì „ì²´ ê³¡ì„  ê¸¸ì´
   ```python
   # ì¸ì ‘ í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ í•©
   L = Î£âˆš((x[i+1] - x[i])Â² + (y[i+1] - y[i])Â²)
   ```

2. **`stress_length(x, y, p, p_thr=2.5)`**: ê³ ì‘ë ¥ êµ¬ê°„ ê¸¸ì´
   ```python
   # ì••ë ¥ > 2.5ì¸ í¬ì¸íŠ¸ë§Œ ì„ íƒí•˜ì—¬ ê¸¸ì´ ê³„ì‚°
   high_stress_points = points[p > 2.5]
   L_stress = calculate_length(high_stress_points)
   ```

3. **`bend_extent(x, y, p)`**: êµ½í˜ ë¹„ìœ¨
   ```python
   # Y ë²”ìœ„ë¥¼ X ë²”ìœ„ë¡œ ë‚˜ëˆ”
   bend_ratio = (y_max - y_min) / (x_max - x_min)
   ```

4. **`max_curvature(x, y, p)`**: ìµœëŒ€ ê³¡ë¥ 
   ```python
   # ê³¡ë¥  ê³µì‹: Îº = |dxÂ·ddy - dyÂ·ddx| / (dxÂ² + dyÂ²)^(3/2)
   curvature = |x'y'' - y'x''| / (x'Â² + y'Â²)^1.5
   max_Îº = max(curvature)
   ```

---

### 3ï¸âƒ£ **ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ**

#### ì§€ì› ëª¨ë¸

1. **CatBoost** (ë²”ì£¼í˜• íŠ¹ì§• ì§ì ‘ ì²˜ë¦¬)
   ```python
   CatBoostClassifier(
       cat_features=['Plant', 'Proc_Param6'],
       verbose=0
   )
   ```

2. **XGBoost** (ê¶Œì¥)
   ```python
   XGBClassifier(
       n_estimators=500,
       scale_pos_weight=1  # í´ë˜ìŠ¤ ê· í˜• ì¡°ì •
   )
   ```

3. **Random Forest**
   ```python
   RandomForestClassifier(
       n_estimators=1200,
       max_features="sqrt",
       bootstrap=True,
       criterion="log_loss"
   )
   ```

#### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

**RandomizedSearchCVë¡œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰**:

```python
param_dist = {
    "n_estimators": [300, 500, 800, 1200],
    "max_features": ["sqrt", "log2", None, 0.1~1.0],
    "max_depth": [None, 5, 10, 15, 20, 25, 30],
    "min_samples_split": [2, 5, 10, 20, 50],
    "min_samples_leaf": [1, 2, 4, 8, 16],
    "bootstrap": [True, False],
    "class_weight": [None, "balanced", "balanced_subsample"],
    "criterion": ["gini", "entropy", "log_loss"]
}
```

**ìµœì  íŒŒë¼ë¯¸í„° (ì˜ˆì‹œ ê²°ê³¼)**:
```python
{
    'n_estimators': 500,
    'min_samples_split': 50,
    'min_samples_leaf': 4,
    'max_features': 'log2',
    'max_depth': 30,
    'criterion': 'log_loss',
    'class_weight': None,
    'bootstrap': False
}
```

---

### 4ï¸âƒ£ **í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (ì„ íƒ ì‚¬í•­)**

```python
def apply_smote(X, y, random_state=42):
    """
    SMOTE (Synthetic Minority Over-sampling Technique)
    ì†Œìˆ˜ í´ë˜ìŠ¤(NG)ì˜ í•©ì„± ìƒ˜í”Œ ìƒì„±ìœ¼ë¡œ ê· í˜• ë§ì¶¤
    """
    smote = SMOTE(random_state=random_state)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    return X_resampled, y_resampled
```

**ì£¼ì˜ì‚¬í•­**:
- SMOTEëŠ” í•™ìŠµ ë°ì´í„°ì—ë§Œ ì ìš©
- ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ì›ë³¸ ë¶„í¬ ìœ ì§€
- Profit ìµœì í™” ì‹œ ì‹¤ì œ ë¶„í¬ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ì‹ ì¤‘íˆ ì‚¬ìš©

---

## ğŸ“ˆ í‰ê°€ ë° ì‹œê°í™”

### 1ï¸âƒ£ **ì„±ëŠ¥ ì§€í‘œ**

#### ê¸°ë³¸ ë¶„ë¥˜ ì§€í‘œ

```python
def print_result(model, X, y_true):
    """
    ëª¨ë¸ ì„±ëŠ¥ ì¶œë ¥:
    - Accuracy: (TP + TN) / Total
    - Recall (TPR): TP / (TP + FN)
    - Precision: TP / (TP + FP)
    - NPV: TN / (TN + FN)  â† íŠ¹íˆ ì¤‘ìš”!
    - F1 Score: 2 Ã— (Recall Ã— Precision) / (Recall + Precision)
    """
```

**NPV (Negative Predictive Value)**:
- **ì •ì˜**: Goodìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ Good ë¹„ìœ¨
- **ì˜ë¯¸**: ì„ íƒí•œ ì œí’ˆì˜ í’ˆì§ˆ ì‹ ë¢°ë„
- **ëª©í‘œ**: NPVë¥¼ ë†’ì—¬ ë¶ˆëŸ‰í’ˆ ì¶œí•˜ ìµœì†Œí™”

---

### 2ï¸âƒ£ **ROC/PRC ê³¡ì„ **

```python
def plot_curves(p_hat, y_true, ax, curve_type='auroc', n=100):
    """
    ë¶„ë¥˜ ì„±ëŠ¥ ê³¡ì„  ì‹œê°í™”

    curve_type:
        - 'auroc': ROC Curve (FPR vs TPR)
        - 'auprc': Precision-Recall Curve
        - 'NPV': NPV vs Threshold
        - 'profit': Profit vs Threshold
    """
```

**AUROC (Area Under ROC Curve)**:
- **ë²”ìœ„**: 0.5 ~ 1.0
- **ì˜ë¯¸**: 0.5 = ëœë¤ ì˜ˆì¸¡, 1.0 = ì™„ë²½í•œ ë¶„ë¥˜
- **ëª©í‘œ**: 0.75 ì´ìƒ

**AUPRC (Area Under Precision-Recall Curve)**:
- ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ ë” ë¯¼ê°í•œ ì§€í‘œ
- Precisionê³¼ Recallì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ì‹œê°í™”

---

### 3ï¸âƒ£ **Profit ê³¡ì„  (í•µì‹¬)**

#### Actual Profit Curve

```python
# ì‹¤ì œ ë ˆì´ë¸” ê¸°ë°˜ Profit ê³„ì‚°
plot_curves(p_hat, y_true, ax, curve_type='profit', n=2000)
```

**ê³„ì‚° ë¡œì§**:
```python
for threshold in [0, 0.001, ..., 1.0]:
    y_pred = (p_hat < threshold)  # Good ì˜ˆì¸¡ (ì„ íƒ)

    TP = (y_pred == 1) & (y_true == 1)  # ë¯¸ì‚¬ìš© (FPì™€ ë™ì¼ ì·¨ê¸‰)
    TN = (y_pred == 1) & (y_true == 0)  # Good ë§ì¶¤ â†’ +100
    FP = (y_pred == 0) & (y_true == 1)  # ë¯¸ì‚¬ìš©
    FN = (y_pred == 0) & (y_true == 0)  # NG ë†“ì¹¨ â†’ -2000

    profit = TN Ã— 100 - FN Ã— 2000 - penalty

    if TN + FN > (200 Ã— N / 466):
        penalty = 99999
```

#### Expected Profit Curve (ì¶”ë¡ ìš©)

```python
plot_profit(p_hat, y_true=None, ax, quantile=[0.05, 0.95], n=1000)
```

**ê¸°ëŒ“ê°’ ê³„ì‚°**:
```python
for threshold in [0, 0.001, ..., 1.0]:
    selected = (p_hat < threshold)
    p_selected = p_hat[selected]

    # ê° ìƒ˜í”Œì˜ ê¸°ëŒ“ê°’
    profit_per_sample = (1 - p_selected) Ã— 100 - p_selected Ã— 2000

    # ì „ì²´ ê¸°ëŒ“ê°’
    expected_profit = profit_per_sample.sum() - penalty

    # í‘œì¤€í¸ì°¨ (ë¶ˆí™•ì‹¤ì„±)
    std_profit = 2100 Ã— âˆš(Î£(p Ã— (1 - p)))
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
Expected Optimal Profit: 12,345(Â±1,234) at thr=0.016
Decision Profit: 11,890 at thr=0.016
```

**ì‹œê°í™” ìš”ì†Œ**:
- **ê²€ì€ìƒ‰ ì‹¤ì„ **: ê¸°ëŒ“ê°’ í‰ê· 
- **íšŒìƒ‰ ì˜ì—­**: ì‹ ë¢°êµ¬ê°„ (5%, 25%, 75%, 95% quantile)
- **ìµœì  ì„ê³„ê°’**: Expected Profitì´ ìµœëŒ€ì¸ ì§€ì 

---

### 4ï¸âƒ£ **Calibration Plot (í™•ë¥  ë³´ì • ê²€ì¦)**

```python
# ì˜ˆì¸¡ í™•ë¥  vs ì‹¤ì œ ë¹„ìœ¨ ë¹„êµ
bins = np.linspace(0, 0.1, 11)
freq_positive = histogram(p_hat[y_true == 1], bins)
freq_negative = histogram(p_hat[y_true == 0], bins)

ratio_actual = freq_positive / (freq_positive + freq_negative)
p_predicted = (bins[1:] + bins[:-1]) / 2

plt.plot(p_predicted, ratio_actual)  # ì´ìƒì ìœ¼ë¡œ y=x
```

**í•´ì„**:
- **y = x ì„  ìœ„**: ëª¨ë¸ì´ í™•ë¥ ì„ ê³¼ì†Œí‰ê°€ (ì•ˆì „)
- **y = x ì„  ì•„ë˜**: ëª¨ë¸ì´ í™•ë¥ ì„ ê³¼ëŒ€í‰ê°€ (ìœ„í—˜)
- **ëª©í‘œ**: ëŒ€ê°ì„ ì— ê°€ê¹ê²Œ (Well-calibrated)

---

### 5ï¸âƒ£ **SHAP Analysis (Feature Importance)**

```python
import shap

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# NG í´ë˜ìŠ¤ì— ëŒ€í•œ ê¸°ì—¬ë„
shap_value_pos = shap_values[:, :, 1]
shap.summary_plot(shap_value_pos, X_test, plot_type="dot")
```

**í•´ì„**:
- **ë¹¨ê°„ìƒ‰ ì **: ë†’ì€ íŠ¹ì§•ê°’ì´ NG ì˜ˆì¸¡ì— ê¸°ì—¬
- **íŒŒë€ìƒ‰ ì **: ë‚®ì€ íŠ¹ì§•ê°’ì´ NG ì˜ˆì¸¡ì— ê¸°ì—¬
- **ìƒìœ„ íŠ¹ì§•**: ê°€ì¥ ì˜í–¥ë ¥ ìˆëŠ” ë³€ìˆ˜

---

## ğŸ”„ Cross-Validation

```python
def cross_validate(model, X, y, cv=5, seed=None):
    """
    K-Fold Cross-Validationìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€

    Parameters:
        cv: Fold ê°œìˆ˜ (ê¸°ë³¸ 5, ë” ì—„ê²©í•œ í‰ê°€ëŠ” 10~15)

    Returns:
        ps: ê° Foldì˜ ì˜ˆì¸¡ í™•ë¥  (cv, n_val)
        ys: ê° Foldì˜ ì‹¤ì œ ë ˆì´ë¸” (cv, n_val)
    """
```

**ì¥ì **:
- ëª¨ë“  ë°ì´í„°ë¥¼ ê²€ì¦ì— í™œìš©
- ê³¼ì í•© ì¡°ê¸° ë°œê²¬
- ëª¨ë¸ ì•ˆì •ì„± í‰ê°€

---

## ğŸ¯ ìµœì¢… ì ìˆ˜ (Final Score)

```python
def final_score(auroc, profit_mean, profit_std):
    """
    ëŒ€íšŒ ìµœì¢… í‰ê°€ ì§€í‘œ

    Formula:
        Score = âˆš(max(AUROC - 0.5, 0) / 0.5 Ã— max(Profit, 0) / 20,000)

    Components:
        - AUROC: ë¶„ë¥˜ ì„±ëŠ¥ (0.5~1.0 ì •ê·œí™”)
        - Profit: ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ (0~20,000 ì •ê·œí™”)

    Range: 0 ~ 1
    """
    return np.sqrt(
        max(auroc - 0.5, 0) / 0.5 *
        max(profit_mean, 0) / 20000
    )
```

**í•´ì„**:
- AUROCì™€ Profitì˜ ê¸°í•˜í‰ê·  (âˆš ì‚¬ìš©)
- ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë‚®ìœ¼ë©´ ì ìˆ˜ í¬ê²Œ í•˜ë½
- **ëª©í‘œ**: AUROC > 0.75, Profit > 10,000 â†’ Score > 0.5

---

## ğŸ“Š ì¶”ë¡  ë° ì œì¶œ

### 1ï¸âƒ£ **í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡**

```python
# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
df_exam = pd.read_csv('data/test.csv')

# 2. ì „ì²˜ë¦¬
X_sum_exam, X_fem_exam, ids = split_data(df_exam, train=False)
X_sum_exam = numerize(X_sum_exam, oe=oe)

# 3. ì˜ˆì¸¡
p_exam = model.predict_proba(X_sum_exam)[:, 1]

# 4. ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì˜ì‚¬ê²°ì •
thr_optimal = 0.016  # Expected Profit ê³¡ì„ ì—ì„œ ê²°ì •
decision = (p_exam < thr_optimal)
```

---

### 2ï¸âƒ£ **ì œì¶œ íŒŒì¼ ìƒì„±**

```python
import datetime

# í˜„ì¬ ì‹œê°„ (KST)
now = datetime.datetime.now(
    tz=datetime.timezone(datetime.timedelta(hours=9))
).strftime("%m-%d-%H-%M")

# ì œì¶œ ì–‘ì‹ ë¡œë“œ
submission = pd.read_csv('data/sample_submission.csv')

# ì˜ˆì¸¡ ê²°ê³¼ í• ë‹¹
submission['probability'] = np.concatenate([p_exam, p_exam])
submission['decision'] = np.concatenate([decision, decision])

# ì €ì¥
submission.to_csv(f"submission_HAIYONG_{now}.csv", index=False)
```

**íŒŒì¼ í˜•ì‹**:
```csv
ID,probability,decision
0,0.0123,True
1,0.0456,True
2,0.0789,False
...
```

---

## ğŸš€ ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ì „ëµ

### 1ï¸âƒ£ **Feature Engineering**

âœ… **êµ¬í˜„ëœ ê¸°ë²•**:
- One-Hot Encoding (ë²”ì£¼í˜• ë³€ìˆ˜)
- FEM í†µê³„ íŠ¹ì§• ì¶”ì¶œ

ğŸ”§ **ì¶”ê°€ ê°€ëŠ¥ ê¸°ë²•**:
- Polynomial Features (ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš©)
- Target Encoding (ë²”ì£¼í˜• ë³€ìˆ˜ â†’ í‰ê·  ë¶ˆëŸ‰ë¥ )
- Time-series features (ê³µì • ìˆœì„œ ì •ë³´)
- Dimensionality Reduction (PCA, t-SNE for FEM data)

---

### 2ï¸âƒ£ **ëª¨ë¸ ì•™ìƒë¸”**

**Stacking**:
```python
# Level 1 ëª¨ë¸ë“¤
models_L1 = [
    XGBClassifier(...),
    CatBoostClassifier(...),
    RandomForestClassifier(...)
]

# Level 2 ë©”íƒ€ ëª¨ë¸
meta_model = LogisticRegression()
```

**Voting**:
```python
from sklearn.ensemble import VotingClassifier

ensemble = VotingClassifier(
    estimators=[
        ('xgb', XGBClassifier(...)),
        ('cat', CatBoostClassifier(...)),
        ('rf', RandomForestClassifier(...))
    ],
    voting='soft'  # í™•ë¥  í‰ê· 
)
```

---

### 3ï¸âƒ£ **ì„ê³„ê°’ ìµœì í™”**

**Grid Search for Threshold**:
```python
thresholds = np.linspace(0.01, 0.05, 100)
profits = []

for thr in thresholds:
    decision = (p_val < thr)
    profit = calculate_profit(decision, y_val)
    profits.append(profit)

thr_optimal = thresholds[np.argmax(profits)]
```

**Bayesian Optimization**:
- ì„ê³„ê°’ì„ ì—°ì† ë³€ìˆ˜ë¡œ ìµœì í™”
- Expected Profit ìµœëŒ€í™” ëª©í‘œ

---

### 4ï¸âƒ£ **í™•ë¥  ë³´ì • (Calibration)**

```python
from sklearn.calibration import CalibratedClassifierCV

# Platt Scaling ë˜ëŠ” Isotonic Regression
calibrated_model = CalibratedClassifierCV(
    model,
    method='sigmoid',  # or 'isotonic'
    cv=5
)

calibrated_model.fit(X_train, y_train)
p_calibrated = calibrated_model.predict_proba(X_test)[:, 1]
```

**íš¨ê³¼**: ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œ ë¹„ìœ¨ê³¼ ì¼ì¹˜ â†’ Expected Profit ì‹ ë¢°ë„ í–¥ìƒ

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
Code_ì†ì£¼í˜¸/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv              # í•™ìŠµ ë°ì´í„°
â”‚   â”œâ”€â”€ test.csv               # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚   â””â”€â”€ sample_submission.csv  # ì œì¶œ ì–‘ì‹
â”œâ”€â”€ documents/
â”‚   â”œâ”€â”€ Architecture_Overview.md      # Hybrid ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¬¸ì„œ
â”‚   â””â”€â”€ Main_Model_Documentation.md   # ë³¸ ë¬¸ì„œ
â”œâ”€â”€ result/
â”‚   â””â”€â”€ {timestamp}_{model_name}.csv  # ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼
â”œâ”€â”€ main copy.ipynb            # ë³¸ ë…¸íŠ¸ë¶
â””â”€â”€ tire_defect_prediction.ipynb  # Hybrid DL ëª¨ë¸ ë…¸íŠ¸ë¶
```

---

## ğŸ”‘ í•µì‹¬ ìš”ì•½

### **Profit ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸**

âœ… **ëª¨ë¸ í•™ìŠµ**
- [ ] AUROC > 0.75 ë‹¬ì„±
- [ ] NPV > 0.95 ë‹¬ì„± (ë‚®ì€ ì„ê³„ê°’ì—ì„œ)
- [ ] Calibration Plot ê²€ì¦

âœ… **ì„ê³„ê°’ ì„¤ì •**
- [ ] Expected Profit ê³¡ì„  ë¶„ì„
- [ ] ìµœì  ì„ê³„ê°’ = 0.015 ~ 0.025 ë²”ìœ„
- [ ] ì„ íƒ ì œí’ˆ ìˆ˜ < 200ê°œ ê¸°ì¤€ í™•ì¸

âœ… **ê²€ì¦**
- [ ] Cross-Validation Score ì•ˆì •ì 
- [ ] SHAPìœ¼ë¡œ feature í•´ì„ ê°€ëŠ¥
- [ ] Test set ì˜ˆì¸¡ ë¶„í¬ í™•ì¸

âœ… **ì œì¶œ**
- [ ] `probability` ì»¬ëŸ¼: 0~1 ë²”ìœ„
- [ ] `decision` ì»¬ëŸ¼: True/False
- [ ] íŒŒì¼ëª…: `submission_HAIYONG_{timestamp}.csv`

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

- **scikit-learn**: https://scikit-learn.org/
- **XGBoost**: https://xgboost.readthedocs.io/
- **CatBoost**: https://catboost.ai/
- **SHAP**: https://shap.readthedocs.io/
- **imbalanced-learn**: https://imbalanced-learn.org/

### ìœ ìš©í•œ ë…¼ë¬¸/ìë£Œ

- *"Calibration of Machine Learning Models"* (2019)
- *"Cost-Sensitive Learning for Imbalanced Classification"* (2020)
- *"SHAP: A Unified Approach to Interpreting Model Predictions"* (2017)

---

## ğŸ“ í•™ìŠµ í¬ì¸íŠ¸

### ì´ í”„ë¡œì íŠ¸ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê²ƒ

1. **ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ì‹¬ ë¨¸ì‹ ëŸ¬ë‹**
   - ë¶„ë¥˜ ì •í™•ë„ë³´ë‹¤ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜(Profit) ìš°ì„ 
   - ë¹„ìš©-í¸ìµ ë¶„ì„ì„ ëª¨ë¸ì— í†µí•©

2. **í™•ë¥  í•´ì„**
   - ì˜ˆì¸¡ í™•ë¥ ì˜ Calibration ì¤‘ìš”ì„±
   - Expected Value ê¸°ë°˜ ì˜ì‚¬ê²°ì •

3. **ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬**
   - SMOTE, Class Weights í™œìš©
   - NPV ê°™ì€ ëŒ€ì•ˆ ì§€í‘œ ì‚¬ìš©

4. **ëª¨ë¸ í•´ì„**
   - SHAPìœ¼ë¡œ ë¸”ë™ë°•ìŠ¤ ëª¨ë¸ ì„¤ëª…
   - Feature Importance ì‹œê°í™”

5. **ì—”ì§€ë‹ˆì–´ë§ ìš°ìˆ˜ ì‚¬ë¡€**
   - ëª¨ë“ˆí™”ëœ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
   - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ í•¨ìˆ˜
   - ìë™í™”ëœ ê²°ê³¼ ì €ì¥

---

**Document Version**: 1.0
**Author**: ì†ì£¼í˜¸
**Last Updated**: 2025-12-08
**Model Type**: XGBoost / CatBoost / Random Forest
**Optimization Target**: Profit Maximization
