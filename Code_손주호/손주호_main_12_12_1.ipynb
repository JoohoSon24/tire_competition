{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/JoohoSon24/tire_competition/blob/main/%EC%86%90%EC%A3%BC%ED%98%B8_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp drive/MyDrive/25F/Tire/5-ai-and-datascience-competition.zip .\n",
    "!mkdir -d data\n",
    "!unzip 5-ai-and-datascience-competition.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "tab10_kwargs = {'cmap': 'tab10', 'vmin': 0, 'vmax': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, train=True):\n",
    "    cols_sim = [f'{ch}{i}' for i in range(256) for ch in ['x', 'y', 'p']]\n",
    "    X_sum = df.drop(columns=cols_sim)\n",
    "    X_fem = df[cols_sim]\n",
    "    if train:\n",
    "      X_sum = X_sum.drop(columns='Class')\n",
    "      y = df['Class']\n",
    "      return X_sum, X_fem, y\n",
    "    else:\n",
    "      X_sum = X_sum.drop(columns='ID')\n",
    "      ids = df['ID']\n",
    "      return X_sum, X_fem, ids\n",
    "\n",
    "def numerize(X_sum, oe=None):\n",
    "  cat_cols = X_sum.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "  num_cols = X_sum.select_dtypes(exclude=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "  train = oe is None\n",
    "\n",
    "  Xn = X_sum[num_cols].values\n",
    "  if train:\n",
    "    oe = OneHotEncoder(sparse_output=False)\n",
    "    Xc = oe.fit_transform(X_sum[cat_cols])\n",
    "  else:\n",
    "    Xc = oe.transform(X_sum[cat_cols])\n",
    "\n",
    "  ohe_cols = oe.get_feature_names_out(cat_cols).tolist()\n",
    "  X = np.concatenate([Xc, Xn], axis=1)\n",
    "  X = pd.DataFrame(data=X, columns=ohe_cols+num_cols, index=X_sum.index)\n",
    "  if train:\n",
    "    return X, oe\n",
    "  else:\n",
    "    return X\n",
    "\n",
    "def apply_smote(X, y, random_state=42):\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_length(x, y, p):\n",
    "    P = np.stack([x, y], axis=-1)\n",
    "    return np.sqrt((np.diff(P, axis=-2) ** 2).sum(axis=-1)).sum(axis=-1)\n",
    "\n",
    "def stress_length(x, y, p, p_thr=2.5):\n",
    "    P = np.stack([x, y], axis=-1)  # (N, l, 2)\n",
    "    mask = (p > p_thr)  # (N, l)\n",
    "\n",
    "    ls = []\n",
    "    for i in range(p.shape[0]):\n",
    "        P_stress = P[i, mask[i], :]  # (l', 2)\n",
    "        l = np.sqrt((np.diff(P_stress, axis=0) ** 2).sum(axis=-1)).sum(axis=-1)\n",
    "        ls.append(l)\n",
    "    ls = np.array(ls)\n",
    "\n",
    "    return ls\n",
    "\n",
    "def bend_extent(x, y, p):\n",
    "    return (y.max(axis=-1) - y.min(axis=-1)) / (x.max(axis=-1) - x.min(axis=-1))\n",
    "\n",
    "def max_curvature(x, y, p):\n",
    "    dx  = np.gradient(x, axis=1)\n",
    "    dy  = np.gradient(y, axis=1)\n",
    "    ddx = np.gradient(dx, axis=1)\n",
    "    ddy = np.gradient(dy, axis=1)\n",
    "\n",
    "    num = np.abs(dx * ddy - dy * ddx)\n",
    "    den = (dx**2 + dy**2)**1.5 + 1e-9\n",
    "\n",
    "    curvature = num / den\n",
    "    max_kappa = np.max(curvature, axis=1)\n",
    "    return max_kappa\n",
    "\n",
    "def FEM_feat(rows, fn=lambda x, y, p: p.max(axis=-1)):\n",
    "    if isinstance(rows, (pd.Series, pd.DataFrame)):\n",
    "        rows = rows.values\n",
    "    x, y, p = np.transpose(rows.reshape(-1, rows.shape[1] // 3, 3), axes=(2, 0, 1))\n",
    "    return fn(x, y, p)\n",
    "\n",
    "def extract_fem(X_fem, fns=lambda x, y, p: p.max(axis=-1), as_df=False):\n",
    "  if isinstance(X_fem, (pd.Series, pd.DataFrame)):\n",
    "    rows = X_fem.values\n",
    "  else:\n",
    "    rows = X_fem\n",
    "\n",
    "  if not isinstance(fns, list):\n",
    "    fns = [fns]\n",
    "\n",
    "  X_fem_new = []\n",
    "  for fn in fns:\n",
    "    feat = FEM_feat(X_fem, fn=fn)\n",
    "    X_fem_new.append(feat)\n",
    "  X_fem_new = np.stack(X_fem_new, axis=1)\n",
    "\n",
    "  if as_df:\n",
    "    X_fem_new = pd.DataFrame(data = X_fem_new, columns=[f'FEM_feat_{i}' for i in range(len(fns))])\n",
    "\n",
    "  return X_fem_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_table(y_pred, y_true):\n",
    "    if y_true.ndim == 2:\n",
    "      y_true = y_true.squeeze(1)\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "    tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def plot_curves(p_hat, y_true, ax, curve_type='auroc', n=100):\n",
    "    if y_true.ndim == 2:\n",
    "      y_true = y_true.squeeze(1)\n",
    "\n",
    "    x, y = [], []\n",
    "    for thr in np.linspace(0, 1, n + 1):\n",
    "      y_pred = (p_hat > thr).astype(int)\n",
    "\n",
    "      tp, tn, fp, fn = conf_table(y_pred, y_true)\n",
    "      if curve_type == 'auroc':\n",
    "        xx = fp / np.clip((tn + fp), 1, np.inf) # FPR\n",
    "        yy = tp / np.clip((tp + fn), 1, np.inf) # Recall, TPR\n",
    "      elif curve_type == 'auprc':\n",
    "        xx = tp / np.clip((tp + fn), 1, np.inf) # Recall, TPR\n",
    "        yy = tp / (tp + fp) if tp + fp > 0 else 1 # Precision\n",
    "      elif curve_type == 'NPV':\n",
    "        xx = thr\n",
    "        yy = tn / (tn + fn) if tn + fn > 0 else 1\n",
    "      elif curve_type == 'profit':\n",
    "        xx = thr\n",
    "        yy = tn * 100 - fn * 2000 - 99999 * int(fn + tn > 200 * len(y_true) / 466)\n",
    "      x.append(xx)\n",
    "      y.append(yy)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    if curve_type in ['auroc', 'auprc']:\n",
    "      score = -((y[1:] + y[:-1]) / 2 * np.diff(x)).sum()\n",
    "      ax.plot([0, 1], [0, 1] if curve_type == 'auroc' else [1, 0], 'k--', alpha=0.3)\n",
    "      xlabel = 'FPR' if curve_type == 'auroc' else 'Recall'\n",
    "      ylabel = 'TPR' if curve_type == 'auroc' else 'Precision'\n",
    "      print(f\"{curve_type.upper()} score: {score:.4f}\")\n",
    "    elif curve_type == 'NPV':\n",
    "      score = y[len(y)//2]\n",
    "      print(f\"NPV at thr=0.5: {score:.4f}\")\n",
    "      xlabel = 'Threshold'\n",
    "      ylabel = 'NPV'\n",
    "    elif curve_type == 'profit':\n",
    "      thr_opt = x[np.argmax(y)]\n",
    "      score = np.max(y)\n",
    "      print(f\"Actual Optimal Profit: {score:.1f} at thr={thr_opt:.4f}\")\n",
    "      xlabel = 'Threshold'\n",
    "      ylabel = 'Profit'\n",
    "\n",
    "    ax.plot(x, y, c='red', lw=1)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(f'{curve_type} curve)')\n",
    "\n",
    "    return score\n",
    "\n",
    "def plot_profit(p_hat, y_true, ax, quantile=[0.25, 0.75], n=100, xlim=None, ylim=None):\n",
    "    x, y_expected_mean, y_expected_std = [], [], []\n",
    "    if y_true is None:\n",
    "      y_true = np.zeros_like(p_hat)\n",
    "    for thr in np.linspace(0, 1, n + 1):\n",
    "      y_pred = (p_hat > thr).astype(int)\n",
    "      p_selected = p_hat[p_hat < thr]\n",
    "\n",
    "      y_bar = (100 * (1 - p_selected) - 2000 * p_selected).sum() - 99999 * int(len(p_selected) > 200 * len(p_hat) / 466)\n",
    "      y_std = 2100 * np.sqrt((p_selected * (1 - p_selected)).sum())\n",
    "\n",
    "      x.append(thr)\n",
    "      y_expected_mean.append(y_bar)\n",
    "      y_expected_std.append(y_std)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y_expected_mean = np.array(y_expected_mean)\n",
    "    y_expected_std = np.array(y_expected_std)\n",
    "\n",
    "    #thr_opt_expected = x[np.argmax(y_expected_mean)]\n",
    "    thr_opt_expected = 0.03\n",
    "    p_max_expected = np.max(y_expected_mean)\n",
    "    p_std_expected = y_expected_std[np.argmax(y_expected_mean)]\n",
    "\n",
    "    y_pred = (p_hat > thr_opt_expected).astype(int)\n",
    "    tp, tn, fp, fn = conf_table(y_pred, y_true)\n",
    "    p_decision = tn * 100 - fn * 2000 - 99999 * int(fn + tn > 200 * len(y_true) / 466)\n",
    "\n",
    "    print(f\"Expected Optimal Profit: {p_max_expected:.1f}(Â±{p_std_expected:.1f}) at thr={thr_opt_expected:.4f}\")\n",
    "    print(f\"Decision Profit: {p_decision:.1f} at thr={thr_opt_expected:.4f}\")\n",
    "\n",
    "    ax.plot(x, y_expected_mean, c='black', lw=1)\n",
    "    y_q0 = y_q = y_expected_mean + norm.ppf(quantile[0]) * y_expected_std\n",
    "    for q in quantile:\n",
    "      y_q = y_expected_mean + norm.ppf(q) * y_expected_std\n",
    "      ax.plot(x, y_q, c='grey', label=f\"{q*100}%\", lw=1)\n",
    "    ax.fill_between(x, y_q0, y_q, fc='gray', alpha=0.3)\n",
    "\n",
    "    if xlim is not None:\n",
    "      ax.set_xlim(xlim)\n",
    "    if ylim is not None:\n",
    "      ax.set_ylim(ylim)\n",
    "\n",
    "    ax.set_xlabel('Decision Threshold')\n",
    "    ax.set_ylabel('Profit')\n",
    "    ax.set_title(f'Profit curve')\n",
    "    plt.legend()\n",
    "\n",
    "    return thr_opt_expected, p_max_expected, p_std_expected, p_decision\n",
    "\n",
    "def print_result(model, X, y_true):\n",
    "    y_pred = model.predict(X)\n",
    "    tp, tn, fp, fn = conf_table(y_pred, y_true)\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    recall = (tp) / (tp + fn)\n",
    "    precision = (tp) / (tp + fp)\n",
    "    npv = (tn) / (tn + fn)\n",
    "    f1 = 2 * (recall * precision) / (recall + precision)\n",
    "\n",
    "    print(f\"Ground truth positive rate: {y_true.mean():.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"NPV: {npv:.4f}\")\n",
    "    print(f\"F1 score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "def final_score(auroc, profit_mean, profit_std):\n",
    "  return np.sqrt(max(auroc - 0.5, 0) / 0.5 * max(profit_mean, 0) / 20000).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_FEM(row, ax):\n",
    "    if isinstance(row, pd.Series):\n",
    "        row = row.values\n",
    "    x, y, p = row.reshape(-1, 3).T\n",
    "    im = ax.scatter(x, y, c=p, cmap='jet', s=0.5, vmin=1.2, vmax=3)\n",
    "    plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# 1. 'data' í´ë” ìƒì„± (ì´ë¯¸ ìžˆìœ¼ë©´ ë¬´ì‹œ)\n",
    "# ì£¼ì˜: '/data'ëŠ” ì‹œìŠ¤í…œ ë£¨íŠ¸ ê²½ë¡œì´ë¯€ë¡œ ê¶Œí•œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. í˜„ìž¬ ìœ„ì¹˜ ê¸°ì¤€ì¸ 'data'ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "print(\"ì•„ëž˜ [íŒŒì¼ ì„ íƒ] ë²„íŠ¼ì„ ëˆŒëŸ¬ train.csvì™€ test.csvë¥¼ ëª¨ë‘ ì„ íƒí•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# 2. ë¡œì»¬ íŒŒì¼ ì—…ë¡œë“œ (train.csv, test.csv ì„ íƒ)\n",
    "uploaded = files.upload()\n",
    "\n",
    "# 3. ì—…ë¡œë“œëœ íŒŒì¼ì„ 'data' í´ë”ë¡œ ì´ë™\n",
    "for filename in uploaded.keys():\n",
    "    # íŒŒì¼ì´ ì´ë¯¸ data í´ë”ì— ìžˆë‹¤ë©´ ë®ì–´ì“°ê¸° ìœ„í•´ ì´ë™ ì „ í™•ì¸\n",
    "    source = filename\n",
    "    destination = os.path.join('data', filename)\n",
    "\n",
    "    # ì´ë™ (shutil.moveëŠ” íŒŒì¼ ì´ë™ ëª…ë ¹ì–´)\n",
    "    shutil.move(source, destination)\n",
    "    print(f\"âœ… {filename} íŒŒì¼ì´ {destination} ìœ„ì¹˜ë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pub = pd.read_csv('data/train.csv')\n",
    "df_exam = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sum, X_fem, cl_ = split_data(df_pub)\n",
    "X_sum_exam, X_fem_exam, ids = split_data(df_exam, train=False)\n",
    "\n",
    "le = LabelBinarizer()\n",
    "cl = le.fit_transform(cl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, X, y, cv=5, seed=None):\n",
    "  if isinstance(y, pd.Series):\n",
    "    y = y.values\n",
    "  if y.ndim == 2:\n",
    "    y = y.squeeze(1)\n",
    "\n",
    "  N = X.shape[0]\n",
    "  index = np.random.permutation(np.arange(N))\n",
    "  X, y = X.iloc[index], y[index]\n",
    "\n",
    "  block_size = N // cv\n",
    "  ps, ys = [], []\n",
    "  for i in range(cv):\n",
    "    s, e = block_size * i, min(block_size * (i + 1), N)\n",
    "    mask = ((np.arange(N) >= s) & (np.arange(N) < e))\n",
    "    X_train, X_val = X.iloc[~mask], X.iloc[mask]\n",
    "    y_train, y_val = y[~mask], y[mask]\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    p = model.predict_proba(X_val)[:, 1]\n",
    "    ps.append(p)\n",
    "    ys.append(y_val)\n",
    "  ps = np.stack(ps)\n",
    "  ys = np.stack(ys)\n",
    "\n",
    "  return ps, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FEM Info. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def collate_fn(batch):\n",
    "  keys = batch[0][-1].keys()\n",
    "  xs = torch.stack([x for x, y, p, l in batch])\n",
    "  ys = torch.stack([y for x, y, p, l in batch])\n",
    "  ps = torch.stack([p for x, y, p, l in batch])\n",
    "  labels = {k: torch.tensor([l[k] for x, y, p, l in batch]).to(torch.float32) for k in keys}\n",
    "  return xs, ys, ps, labels\n",
    "\n",
    "class FEMDataset(Dataset):\n",
    "  def __init__(self, df, labels):\n",
    "    self.df = df\n",
    "    labels = (labels - labels.mean()) / labels.std()\n",
    "    self.labels = {col: labels[col].values for col in labels.columns}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "    row = self.df.iloc[i].values\n",
    "    x, y, p = row.reshape(-1, 3).T\n",
    "    x = torch.from_numpy(x).to(torch.float32)\n",
    "    y = torch.from_numpy(y).to(torch.float32)\n",
    "    p = torch.from_numpy(p).to(torch.float32)\n",
    "\n",
    "    x = (x - x[0]) / 100\n",
    "    y = (y - y[0]) / 20\n",
    "\n",
    "    l = {k: v[i] for k, v in self.labels.items()}\n",
    "    return x, y, p, l\n",
    "\n",
    "  def get_labels(self):\n",
    "    return self.labels\n",
    "\n",
    "class FEMEncoder(nn.Module):\n",
    "  def __init__(self, in_dim=256*3, hid_dim=256, rep_dim=16, p=0.1, targets: list = ['G1', 'G2', 'G3']):\n",
    "    super().__init__()\n",
    "    self.body = nn.Sequential(\n",
    "        nn.Linear(in_dim, hid_dim),\n",
    "        nn.Dropout(p=p),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hid_dim, rep_dim),\n",
    "        nn.Dropout(p=p),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    heads = {}\n",
    "    for k in targets:\n",
    "      heads[k] = nn.Linear(rep_dim, 1)\n",
    "    self.heads = nn.ModuleDict(heads)\n",
    "    self.targets = list(self.heads.keys())\n",
    "\n",
    "  def get_repv(self, x, y, p):\n",
    "    h = torch.cat([x, y, p], dim=-1)\n",
    "    return self.body(h)\n",
    "\n",
    "  def forward(self, x, y, p):\n",
    "    h = self.get_repv(x, y, p)\n",
    "    ys = {}\n",
    "    for k in self.targets:\n",
    "      ys[k] = self.heads[k](h).squeeze(-1)\n",
    "    return ys\n",
    "\n",
    "class CNNEncoder(FEMEncoder):\n",
    "    def __init__(self, in_dim=3, hid_dim=64, rep_dim=16, n_blocks=4, p=.1, targets: list = ['G1', 'G2', 'G3']):\n",
    "        \"\"\"\n",
    "        channels: ìž…ë ¥ ì±„ë„ ìˆ˜ (x, y, p â†’ 3)\n",
    "        hid_dim: Conv hidden size\n",
    "        layers: dilated residual block ê°œìˆ˜\n",
    "        \"\"\"\n",
    "        super().__init__(in_dim, hid_dim, rep_dim, p, targets)\n",
    "        del self.body\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        in_channels = in_dim\n",
    "        out_channels = hid_dim\n",
    "        dilation = 1\n",
    "        for _ in range(n_blocks):\n",
    "            padding = dilation\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=3,\n",
    "                          dilation=dilation, padding=padding),\n",
    "                nn.Dropout(p=p),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.convs.append(block)\n",
    "            in_channels = out_channels\n",
    "            out_channels = hid_dim\n",
    "            dilation *= 2\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, rep_dim)\n",
    "        )\n",
    "\n",
    "    def get_repv(self, x, y, p):\n",
    "        \"\"\"\n",
    "        x, y, p: ê°ê° shape (batch, T)\n",
    "        return: z (batch, hidden_dim)\n",
    "        \"\"\"\n",
    "        # (B, T) â†’ (B, 1, T)\n",
    "        x = x.unsqueeze(1)\n",
    "        y = y.unsqueeze(1)\n",
    "        p = p.unsqueeze(1)\n",
    "\n",
    "        # concat â†’ (B, 3, T)\n",
    "        h = torch.cat([x, y, p], dim=1)\n",
    "\n",
    "        # CNN forward\n",
    "        for conv in self.convs:\n",
    "            h = conv(h)\n",
    "\n",
    "        z = h.mean(dim=-1)\n",
    "\n",
    "        # MLP â†’ final representation\n",
    "        z = self.mlp(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_medusa(preds, trues, weight=None):\n",
    "  assert preds.keys() == trues.keys()\n",
    "\n",
    "  loss = torch.zeros(1, device=list(preds.values())[0].device)\n",
    "  for k in trues.keys():\n",
    "    pred, true = preds[k], trues[k]\n",
    "    assert pred.device == true.device, f\"pred: {pred.device} / true: {true.device}\"\n",
    "    loss += nn.functional.mse_loss(pred, true)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED: Add Class label to DNN training targets\n",
    "X_fem_all = pd.concat([X_fem, X_fem_exam], axis=0).reset_index(drop=True)\n",
    "X_sum_all = pd.concat([X_sum, X_sum_exam], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Add Class column: train data has actual labels, exam data has dummy 0s\n",
    "cl_all = pd.Series(\n",
    "    np.concatenate([cl_.values, np.zeros(len(X_sum_exam))]),\n",
    "    name='Class'\n",
    ")\n",
    "X_sum_all_with_class = pd.concat([X_sum_all, cl_all], axis=1)\n",
    "\n",
    "N = len(X_fem_all)\n",
    "r = .8\n",
    "max_epoch = 100\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# MODIFIED: Add 'Class' to target_cols for DNN training\n",
    "target_cols = ['Y1', 'G1', 'X1', 'Y5', 'X5', 'Y2', 'G2', 'G4', 'G3', 'Y3', 'Class']\n",
    "loss_fn = loss_medusa\n",
    "\n",
    "indices = torch.randperm(N)\n",
    "\n",
    "train_set = FEMDataset(X_fem_all.iloc[indices[:int(N*r)]], labels=X_sum_all_with_class[target_cols].iloc[indices[:int(N*r)]])\n",
    "val_set = FEMDataset(X_fem_all.iloc[indices[int(N*r):]], labels=X_sum_all_with_class[target_cols].iloc[indices[int(N*r):]])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fem = CNNEncoder(hid_dim=256, targets=target_cols)\n",
    "for batch in val_loader:\n",
    "  x, y, p, ls = batch\n",
    "  ls_pred = model_fem(x, y, p)\n",
    "  print({k: v.shape for k, v in ls_pred.items()}); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fem = CNNEncoder(hid_dim=128, rep_dim=12, p=0.1, targets=target_cols)\n",
    "optimizer = Adam(params=model_fem.parameters(), lr=1e-3, weight_decay=2e-4)\n",
    "\n",
    "def run_epoch(train, epoch, device, verbose=True):\n",
    "  loader = train_loader if train else val_loader\n",
    "  epoch_loss = []\n",
    "  bar = tqdm(loader) if verbose else loader\n",
    "  for batch in bar:\n",
    "    x, y, p, labels = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    p = p.to(device)\n",
    "    for k in labels.keys():\n",
    "      labels[k] = labels[k].to(device)\n",
    "\n",
    "    labels_pred = model_fem(x, y, p)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    if train:\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "    epoch_loss.append(loss.item())\n",
    "  epoch_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "  return epoch_loss\n",
    "\n",
    "model_size = sum([param.numel() for param in model_fem.parameters()])\n",
    "print(f\"Model size: {model_size}\")\n",
    "\n",
    "loss_dict = {'train': [], 'val': []}\n",
    "model_fem.to(device)\n",
    "for i in range(max_epoch):\n",
    "  verbose = (i % 10 == 0)\n",
    "  model_fem.train()\n",
    "  train_loss = run_epoch(train=True, epoch=i, device=device, verbose=verbose)\n",
    "  loss_dict['train'].append(train_loss)\n",
    "  print(f\"Epoch {i} training loss: {train_loss:.6f}\") if verbose else None\n",
    "\n",
    "  model_fem.eval()\n",
    "  with torch.no_grad():\n",
    "    val_loss = run_epoch(train=False, epoch=i, device=device, verbose=verbose)\n",
    "  loss_dict['val'].append(val_loss)\n",
    "  print(f\"Epoch {i} validation loss: {val_loss:.6f}\") if verbose else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = val_set.get_labels()\n",
    "labels_pred = {k: [] for k in target_cols}\n",
    "model_fem.eval()\n",
    "for batch in val_loader:\n",
    "  x, y, p, _ = batch\n",
    "  x = x.to(device)\n",
    "  y = y.to(device)\n",
    "  p = p.to(device)\n",
    "  with torch.no_grad():\n",
    "    out = model_fem(x, y, p)\n",
    "    for k in target_cols:\n",
    "      labels_pred[k].append(out[k])\n",
    "\n",
    "for k in target_cols:\n",
    "  labels_pred[k] = torch.cat(labels_pred[k]).detach().cpu().numpy()\n",
    "\n",
    "r, c = 6, 2\n",
    "fig, axes = plt.subplots(r, c, figsize=(6 * c, 4 * r))\n",
    "\n",
    "axes[0, 0].plot(loss_dict['train'], label='train')\n",
    "axes[0, 0].plot(loss_dict['val'], label='val')\n",
    "axes[0, 0].set_title(\"Loss plot\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "for n, k in enumerate(target_cols):\n",
    "  i, j = (n + 1) // c, (n + 1) % c\n",
    "  ax = axes[i, j]\n",
    "  label, label_pred = labels[k], labels_pred[k]\n",
    "  m, M = np.sort(np.concatenate([label, label_pred]))[[0, -1]]\n",
    "\n",
    "  ax.scatter(label, label_pred, s=5)\n",
    "  ax.plot([m, M], [m, M], lw=0.5, c='black')\n",
    "  ax.set_xlim(m - (M - m) * 0.05, M + (M - m) * 0.05)\n",
    "  ax.set_ylim(m - (M - m) * 0.05, M + (M - m) * 0.05)\n",
    "\n",
    "  SST = ((label - label.mean()) ** 2).sum()\n",
    "  SSE = ((label_pred - label) ** 2).sum()\n",
    "  R_sq =  1 - SSE / SST\n",
    "  ax.set_title(f\"Feature {k} - R sq.: {R_sq:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED: Add Class to labels when extracting train FEM features\n",
    "X_sum_with_class = pd.concat([X_sum, pd.Series(cl_.values, name='Class')], axis=1)\n",
    "pub_set = FEMDataset(X_fem, labels=X_sum_with_class[target_cols])\n",
    "pub_loader = DataLoader(pub_set, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "rep_vs = []\n",
    "model_fem.eval()\n",
    "for batch in pub_loader:\n",
    "  x, y, p, _ = batch\n",
    "  x = x.to(device)\n",
    "  y = y.to(device)\n",
    "  p = p.to(device)\n",
    "  with torch.no_grad():\n",
    "    rep_vs.append(model_fem.get_repv(x, y, p))\n",
    "rep_vs = torch.cat(rep_vs, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fem_feat = pd.DataFrame(\n",
    "    data = rep_vs,\n",
    "    columns = [f'FEM_feat_{i}' for i in range(rep_vs.shape[1])]\n",
    ")\n",
    "df_fem_feat.to_csv('data/fem_ext.csv')\n",
    "df_fem_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(X_sum):\n",
    "    \"\"\"\n",
    "    Complete Feature Engineering Pipeline.\n",
    "    Should be applied BEFORE numerize() to preserve categorical columns!\n",
    "\n",
    "    Args:\n",
    "        X_sum: DataFrame from split_data() with tabular features\n",
    "\n",
    "    Returns:\n",
    "        X_sum: DataFrame with engineered features\n",
    "    \"\"\"\n",
    "    df = X_sum.copy()\n",
    "\n",
    "    # === 1. Spec Engineering: íƒ€ì´ì–´ ì œì› ê¸°ë°˜ ë¬¼ë¦¬ëŸ‰ ê³„ì‚° ===\n",
    "    # Sidewall Height (ë‹¨ë©´ ë†’ì´ mm) = ë‹¨ë©´í­ * íŽ¸í‰ë¹„ / 100\n",
    "    df['Feat_Sidewall_H'] = df['Width'] * (df['Aspect'] / 100)\n",
    "\n",
    "    # Total Diameter (íƒ€ì´ì–´ ì „ì²´ ì§€ë¦„ mm) = (ë‹¨ë©´ ë†’ì´ * 2) + (íœ  ì¸ì¹˜ * 25.4)\n",
    "    df['Feat_Diameter'] = (df['Feat_Sidewall_H'] * 2) + (df['Inch'] * 25.4)\n",
    "\n",
    "    # Tire Volume Proxy (íƒ€ì´ì–´ ë¶€í”¼ ê·¼ì‚¬ì¹˜) = ì§€ë¦„ * ë‹¨ë©´í­\n",
    "    df['Feat_Volume_Proxy'] = df['Feat_Diameter'] * df['Width']\n",
    "\n",
    "    # Aspect Ratio Interaction (ì¸ì¹˜ ëŒ€ë¹„ í­ ë¹„ìœ¨)\n",
    "    df['Feat_Width_to_Inch'] = df['Width'] / (df['Inch'] + 1e-6)\n",
    "\n",
    "    # === 2. Geometry Engineering: í˜•ìƒ ë²¡í„° ê¸°í•˜í•™ì  ì²˜ë¦¬ ===\n",
    "    x_cols = [f'X{i}' for i in range(1, 6)]\n",
    "    y_cols = [f'Y{i}' for i in range(1, 6)]\n",
    "\n",
    "    if all(col in df.columns for col in x_cols + y_cols):\n",
    "        # ì›ì ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬\n",
    "        dists = []\n",
    "        for xc, yc in zip(x_cols, y_cols):\n",
    "            dists.append(np.sqrt(df[xc]**2 + df[yc]**2))\n",
    "\n",
    "        df['Feat_Geo_Mean_Dist'] = np.mean(dists, axis=0)\n",
    "        df['Feat_Geo_Max_Dist'] = np.max(dists, axis=0)\n",
    "        df['Feat_Geo_Std_Dist'] = np.std(dists, axis=0)\n",
    "\n",
    "        # í˜•ìƒ ë¹„ëŒ€ì¹­ì„±\n",
    "        x_range = df[x_cols].max(axis=1) - df[x_cols].min(axis=1)\n",
    "        y_range = df[y_cols].max(axis=1) - df[y_cols].min(axis=1)\n",
    "        df['Feat_Geo_Aspect_Ratio'] = x_range / (y_range + 1e-6)\n",
    "\n",
    "        # Shoelace formula for polygon area\n",
    "        area = 0\n",
    "        for i in range(5):\n",
    "            j = (i + 1) % 5\n",
    "            term = df[f'X{i+1}'] * df[f'Y{j+1}'] - df[f'X{j+1}'] * df[f'Y{i+1}']\n",
    "            area += term\n",
    "        df['Feat_Geo_Area'] = np.abs(area) / 2.0\n",
    "\n",
    "    # === 3. Process Param Engineering: ê³µì • íŒŒë¼ë¯¸í„° í†µê³„ ===\n",
    "    proc_cols = [c for c in df.columns if 'Proc_Param' in c]\n",
    "    if proc_cols:\n",
    "        df['Feat_Proc_Mean'] = df[proc_cols].mean(axis=1)\n",
    "        df['Feat_Proc_Std'] = df[proc_cols].std(axis=1)\n",
    "        df['Feat_Proc_Max'] = df[proc_cols].max(axis=1)\n",
    "        df['Feat_Proc_Min'] = df[proc_cols].min(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"âœ… Feature Engineering í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"   - engineer_features(X_sum) í•¨ìˆ˜ë¥¼ numerize() ì „ì— í˜¸ì¶œí•˜ì„¸ìš”!\")\n",
    "print(\"   - Train/Test ëª¨ë‘ ë™ì¼í•˜ê²Œ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "import shap\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING PIPELINE START\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration - MODIFIED: Changed to CatBoost\n",
    "model_type = 'catboost'  # 'catboost' 'xgb' 'rf'\n",
    "include_fem = True\n",
    "augment = False\n",
    "\n",
    "# === STEP 1: Feature Engineering (BEFORE numerize!) ===\n",
    "print(\"\\n[1/4] Applying Feature Engineering...\")\n",
    "X_sum_engineered = engineer_features(X_sum)\n",
    "print(f\"   âœ“ Original features: {X_sum.shape[1]}\")\n",
    "print(f\"   âœ“ After engineering: {X_sum_engineered.shape[1]}\")\n",
    "\n",
    "# === STEP 2: Model & Encoding ===\n",
    "print(\"\\n[2/4] Configuring model and encoding...\")\n",
    "if model_type == 'catboost':\n",
    "    model = CatBoostClassifier(\n",
    "        cat_features=['Plant', 'Proc_Param6'],\n",
    "        verbose=0,\n",
    "    )\n",
    "    X = X_sum_engineered  # CatBoost handles categorical directly\n",
    "\n",
    "elif model_type == 'xgb':\n",
    "    model = XGBRFClassifier(\n",
    "        n_estimators=1000,\n",
    "        scale_pos_weight=1,\n",
    "    )\n",
    "    X, oe = numerize(X_sum_engineered)\n",
    "    print(f\"   âœ“ Encoded features: {X.shape[1]}\")\n",
    "\n",
    "elif model_type == 'rf':\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_features=\"sqrt\",\n",
    "        bootstrap=True,\n",
    "        criterion=\"log_loss\"\n",
    "    )\n",
    "    X, oe = numerize(X_sum_engineered)\n",
    "    print(f\"   âœ“ Encoded features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"   âœ“ Model: {model_type.upper()}\")\n",
    "\n",
    "# === STEP 3: Add FEM features if needed ===\n",
    "print(\"\\n[3/4] Processing FEM features...\")\n",
    "if include_fem:\n",
    "    X_fem_ext = df_fem_feat\n",
    "    # IMPORTANT: Align indices before concat!\n",
    "    X_fem_ext.index = X.index\n",
    "    X = pd.concat([X, X_fem_ext], axis=1)\n",
    "    print(f\"   âœ“ FEM features added. Total: {X.shape[1]} features\")\n",
    "else:\n",
    "    print(f\"   âœ“ No FEM features. Total: {X.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nâœ… Pipeline complete! Final training data shape: {X.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if include_fem:\n",
    "  X_fem_ext = df_fem_feat\n",
    "  X = pd.concat([X, X_fem_ext], axis=1)\n",
    "\n",
    "X_train, X_test, cl_train, cl_test = train_test_split(X, cl, train_size=0.8)\n",
    "if augment:\n",
    "  X_train, cl_train = apply_smote(X_train, cl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, cl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_value_pos = shap_values[:, :, 0]\n",
    "shap.summary_plot(shap_value_pos, X_test, plot_type=\"dot\", show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, ys = cross_validate(model, X, cl, cv=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ps.flatten()\n",
    "y = ys.flatten()\n",
    "mask = (y == 0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "upper = 0.1\n",
    "bins = np.linspace(0, upper, 11)\n",
    "freq_n, _, im1 = axes[0].hist(p[mask], bins=bins, alpha=0.7)\n",
    "freq_p, _, im2 = axes[0].hist(p[~mask], bins=bins, alpha=0.7)\n",
    "\n",
    "acc_n = np.cumsum(freq_n)\n",
    "acc_p = np.cumsum(freq_p)\n",
    "\n",
    "p_pred = (bins[1:] + bins[:-1]) / 2\n",
    "\n",
    "ratio = freq_p / (freq_n + freq_p)\n",
    "\n",
    "bin_indices = np.digitize(p, bins)\n",
    "bin_indices = np.clip(bin_indices, 1, len(bins) - 1) - 1\n",
    "p_bin = ratio[bin_indices]\n",
    "\n",
    "axes[1].plot(p_pred, ratio)\n",
    "axes[1].scatter(p, p_bin, s=3, alpha=0.5, c=y, **tab10_kwargs)\n",
    "axes[1].set_xlim(-upper * 0.1, upper * 1.1)\n",
    "axes[1].set_ylim(-upper * 0.1, upper * 1.1)\n",
    "axes[1].plot([0, upper], [0, upper], c='grey', lw=0.7, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "auroc = plot_curves(p, y, axes[0, 0], curve_type='auroc', n=2000)\n",
    "auprc = plot_curves(p, y, axes[0, 1], curve_type='auprc', n=2000)\n",
    "npv = plot_curves(p, y, axes[1, 0], curve_type='NPV', n=2000)\n",
    "p_actual = plot_curves(p, y, axes[1, 1], curve_type='profit', n=2000)\n",
    "thr_opt, p_mean, p_std, p_dec = plot_profit(p, y, axes[1, 1], quantile=[0.05, 0.25, 0.75, 0.95], n=1000, xlim=(0, 0.1), ylim=(-10000, 21000))\n",
    "print(f\"final score (predicted): {final_score(auroc, p_mean, p_std):.4f}\")\n",
    "print(f\"final score (actual upper bound): {final_score(auroc, p_actual, p_std):.4f}\")\n",
    "print(f\"final score (decision): {final_score(auroc, p_dec, p_std):.4f}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED: Add Class dummy values for exam data\n",
    "X_sum_exam_with_class = pd.concat([X_sum_exam, pd.Series(np.zeros(len(X_sum_exam)), name='Class')], axis=1)\n",
    "pub_set_exam = FEMDataset(X_fem_exam, labels=X_sum_exam_with_class[target_cols])\n",
    "pub_loader_exam = DataLoader(pub_set_exam, batch_size=16, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "rep_vs_exam = []\n",
    "model_fem.eval()\n",
    "for batch in pub_loader_exam:\n",
    "  x, y, p, _ = batch\n",
    "  x = x.to(device)\n",
    "  y = y.to(device)\n",
    "  p = p.to(device)\n",
    "  with torch.no_grad():\n",
    "    rep_vs_exam.append(model_fem.get_repv(x, y, p))\n",
    "rep_vs_exam = torch.cat(rep_vs_exam, dim=0).detach().cpu().numpy()\n",
    "\n",
    "df_fem_feat_exam = pd.DataFrame(\n",
    "    data = rep_vs_exam,\n",
    "    columns = [f'FEM_feat_{i}' for i in range(rep_vs.shape[1])]\n",
    ")\n",
    "df_fem_feat_exam.to_csv('data/fem_ext.csv')\n",
    "df_fem_feat_exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXAM DATA PREDICTION & SUBMISSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === STEP 1: Retrain on full data ===\n",
    "print(\"\\n[1/5] Retraining model on full training data...\")\n",
    "model.fit(X, cl)\n",
    "print(\"   âœ“ Model retrained\")\n",
    "\n",
    "# === STEP 2: Apply EXACT SAME pipeline to exam data ===\n",
    "print(\"\\n[2/5] Processing exam data with same pipeline...\")\n",
    "\n",
    "# 2-1: Feature Engineering (BEFORE numerize!)\n",
    "print(\"   - Applying Feature Engineering...\")\n",
    "X_sum_exam_engineered = engineer_features(X_sum_exam)\n",
    "print(f\"     âœ“ Exam features: {X_sum_exam.shape[1]} â†’ {X_sum_exam_engineered.shape[1]}\")\n",
    "\n",
    "# 2-2: Encoding with fitted encoder\n",
    "print(\"   - Encoding categorical features...\")\n",
    "if model_type == 'catboost':\n",
    "    X_exam = X_sum_exam_engineered\n",
    "else:\n",
    "    X_exam = numerize(X_sum_exam_engineered, oe=oe)  # Use fitted oe!\n",
    "print(f\"     âœ“ Encoded features: {X_exam.shape[1]}\")\n",
    "\n",
    "# 2-3: Add FEM features\n",
    "print(\"   - Adding FEM features...\")\n",
    "if include_fem:\n",
    "    X_fem_ext_exam = df_fem_feat_exam\n",
    "    # IMPORTANT: Align indices before concat!\n",
    "    X_fem_ext_exam.index = X_exam.index\n",
    "    X_exam = pd.concat([X_exam, X_fem_ext_exam], axis=1)\n",
    "    print(f\"     âœ“ FEM features added. Total: {X_exam.shape[1]} features\")\n",
    "\n",
    "print(f\"   âœ“ Final exam data shape: {X_exam.shape}\")\n",
    "\n",
    "# === STEP 3: Verify column match ===\n",
    "print(\"\\n[3/5] Verifying column consistency...\")\n",
    "if X.shape[1] == X_exam.shape[1]:\n",
    "    print(f\"   âœ… Column count matches! Train: {X.shape[1]}, Exam: {X_exam.shape[1]}\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  WARNING: Column mismatch! Train: {X.shape[1]}, Exam: {X_exam.shape[1]}\")\n",
    "    print(\"   This will cause prediction errors!\")\n",
    "\n",
    "# === STEP 4: Predict ===\n",
    "print(\"\\n[4/5] Generating predictions...\")\n",
    "p_exam = model.predict_proba(X_exam)[:, 1]\n",
    "print(f\"   - Predictions generated: {len(p_exam)} samples\")\n",
    "print(f\"   - Probability range: [{p_exam.min():.4f}, {p_exam.max():.4f}]\")\n",
    "\n",
    "# Profit curve analysis\n",
    "print(\"\\n   - Analyzing profit curve...\")\n",
    "ax = plt.subplot()\n",
    "thr, p_mean, p_std, _ = plot_profit(\n",
    "    p_hat=p_exam,\n",
    "    y_true=None,\n",
    "    ax=ax,\n",
    "    n=1000,\n",
    "    xlim=(0, 0.1),\n",
    "    ylim=(-10000, 10000)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"   âœ“ Optimal threshold: {thr:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUBMISSION FILE GENERATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === STEP 1: Use optimal threshold from profit curve ===\n",
    "print(f\"\\n[1/3] Using optimal threshold from profit curve: {thr:.4f}\")\n",
    "decision = (p_exam < thr)\n",
    "print(f\"   âœ“ Selected products: {decision.sum()} / {len(decision)}\")\n",
    "print(f\"   âœ“ Selection rate: {decision.sum()/len(decision)*100:.1f}%\")\n",
    "\n",
    "# === STEP 2: Load sample submission template ===\n",
    "print(\"\\n[2/3] Creating submission DataFrame...\")\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# For L and R tires (466 each = 932 total), concatenate predictions\n",
    "submission['probability'] = np.concatenate([p_exam, p_exam])\n",
    "submission['decision'] = np.concatenate([decision, decision]).astype(int)\n",
    "\n",
    "print(f\"   âœ“ Submission shape: {submission.shape}\")\n",
    "print(f\"   âœ“ Columns: {submission.columns.tolist()}\")\n",
    "print(f\"   âœ“ Total selected: {submission['decision'].sum()}\")\n",
    "\n",
    "# === STEP 3: Save submission file ===\n",
    "print(\"\\n[3/3] Saving submission file...\")\n",
    "\n",
    "# Create results folder if it doesn't exist\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "    print(\"   âœ“ Created 'results/' folder\")\n",
    "\n",
    "# Generate filename with timestamp\n",
    "now = datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=9))).strftime(\"%m-%d-%H-%M\")\n",
    "filename = f\"results/submission_HAIYONG_{now}.csv\"\n",
    "\n",
    "# Save without index\n",
    "submission.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"   âœ“ File saved: {filename}\")\n",
    "print(f\"\\nðŸ“Š Submission Preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… SUBMISSION FILE CREATED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“ Submit this file: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
